{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Multiclass Classification Problem with SoftMax and Bokeh.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"OAJa_FwAkiqp"},"source":["# Movie genre multiclass classification problem using softmax and bookeh\n","\n","Firsts things firsts, this notebook will use the data distributed by Kaggle: https://www.kaggle.com/shainy/twitter-reviews-for-emotion-analysis\n","<br />\n","Resuming, it's about movie genre\n","<br /><br />\n","In this notebook, we will try to predict the movie genre using the **softmax function** and after, plotting using a bokeh graph"]},{"cell_type":"markdown","metadata":{"id":"8fQKQQxZmDlC"},"source":["----------------------------------------------------------------------------------------------------------------------------------------------------------------"]},{"cell_type":"markdown","metadata":{"id":"2Os3Kj6MmGRU"},"source":["## Beginning of the work\n","\n","To begin, we will mount the connection with drive so we can acess the csv"]},{"cell_type":"code","metadata":{"id":"7RyJhUqGimUc","executionInfo":{"status":"ok","timestamp":1603843425848,"user_tz":180,"elapsed":40027,"user":{"displayName":"Carlos Reinheimer","photoUrl":"","userId":"11551777167550877658"}},"outputId":"b0f9ec37-bc29-45bf-86fb-f4a9a08ba131","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mndyedcdmYew"},"source":["Besides, we will import and use **sys**, so we can import the functions that we are going to use of a **.py** file"]},{"cell_type":"code","metadata":{"id":"qlbE-71empwf","executionInfo":{"status":"ok","timestamp":1603843425849,"user_tz":180,"elapsed":40018,"user":{"displayName":"Carlos Reinheimer","photoUrl":"","userId":"11551777167550877658"}}},"source":["import sys\n","sys.path.append('/content/drive/My Drive/Tarefinha de Data Science/Arquivos - Carlos')"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2ToRyRnNkhQM"},"source":["## Imports\n","\n","Before starting to use the data, we will import all the things we need\n"]},{"cell_type":"code","metadata":{"id":"5a_PZcxTnFpO","executionInfo":{"status":"ok","timestamp":1603843451849,"user_tz":180,"elapsed":7311,"user":{"displayName":"Carlos Reinheimer","photoUrl":"","userId":"11551777167550877658"}},"outputId":"5ee8a8e2-ce7f-407c-cbbc-7d08d9373e8e","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# dataset\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline \n","# pd.set_option('display.max_colwidth', -1)\n","\n","# my functions\n","import UtilsCarlos\n","from UtilsCarlos import  criaDicio, criaVetor, fit_and_score, plot_conf_mat, convert\n","\n","# nltk\n","import nltk\n","from nltk.tokenize import TweetTokenizer\n","import re\n","from nltk import FreqDist\n","from nltk.tokenize import TweetTokenizer\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords \n","tknzr = TweetTokenizer()\n","stemmer = PorterStemmer()\n","nltk.download('punkt')\n","\n","# modelos\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn import tree\n","\n","# evaluations\n","from sklearn.model_selection import train_test_split, cross_val_score\n","from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n","from sklearn.metrics import confusion_matrix, classification_report\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","from sklearn.metrics import plot_roc_curve\n","from sklearn.model_selection import cross_val_score\n","\n","# others\n","import json\n","import string\n","\n","# PyTorch\n","import torch\n","from torch import nn\n","\n","# gensin\n","from gensim.models import KeyedVectors"],"execution_count":3,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MQAebXvbOZia","executionInfo":{"status":"ok","timestamp":1603843456688,"user_tz":180,"elapsed":724,"user":{"displayName":"Carlos Reinheimer","photoUrl":"","userId":"11551777167550877658"}},"outputId":"8f7f9e8b-aabd-4214-a9d9-e142e19ee8df","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["nltk.download('stopwords')\n","stopwords = nltk.corpus.stopwords.words(\"english\")\n","token_espaço = nltk.tokenize.WhitespaceTokenizer()"],"execution_count":4,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VQMaxZrvNbt9"},"source":["---------------------------------------------------------------------------------------------------------"]},{"cell_type":"code","metadata":{"id":"XHLCEe5iONyu","executionInfo":{"status":"ok","timestamp":1603843459040,"user_tz":180,"elapsed":882,"user":{"displayName":"Carlos Reinheimer","photoUrl":"","userId":"11551777167550877658"}}},"source":["# função de limpar texto\n","def cleanText(words, stem=False):\n","  \"\"\"\n","    Esta função recebe um text e retorna o mesmo, já tratado com stopwords & punctuation\n","  \"\"\"\n","  newWords = list()\n","  pontuacao = string,\n","  for word in words:\n","    word = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n","                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', word)\n","    words = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", word)\n","    if len(word) > 0 and words not in string.punctuation and word.lower() not in stopwords and word.lower != \"<br />\":\n","      if stem:\n","        word = stemmer.stem(word.lower())\n","        newWords.append(word)\n","      else:\n","        newWords.append(word.lower())\n","\n","  return newWords"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"QMEdrkjdOOmy","executionInfo":{"status":"ok","timestamp":1603843459042,"user_tz":180,"elapsed":879,"user":{"displayName":"Carlos Reinheimer","photoUrl":"","userId":"11551777167550877658"}}},"source":["# confusion matrix daora\n","def plot_conf_mat(y_test, y_preds, norm=\"false\"):\n","    fig, ax = plt.subplots(figsize=(3, 3))\n","    ax = sns.heatmap(confusion_matrix(y_test, y_preds, normalize=norm),\n","                    annot=True,\n","                    cbar=False)\n","    plt.xlabel(\"True label\")\n","    plt.ylabel(\"Predicted label\")"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"T7kT0ZfyYt0s","executionInfo":{"status":"ok","timestamp":1603843460103,"user_tz":180,"elapsed":619,"user":{"displayName":"Carlos Reinheimer","photoUrl":"","userId":"11551777167550877658"}}},"source":["def plot_loss_and_accuracy(losses, accs):\n","\n","  fig, ax_tuple = plt.subplots(1, 2, figsize=(16,6))\n","  fig.suptitle('Loss and accuracy')\n","\n","  for i, (y_label, y_values) in enumerate(zip(['BCE loss','Accuracy'],[losses, accs])):\n","    ax_tuple[i].plot(range(len(y_values)),  y_values, label='train')\n","    ax_tuple[i].set_xlabel('epochs')\n","    ax_tuple[i].set_ylabel(y_label)\n","    ax_tuple[i].legend()"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"iAKxRC_WV9Ox","executionInfo":{"status":"ok","timestamp":1603843460403,"user_tz":180,"elapsed":784,"user":{"displayName":"Carlos Reinheimer","photoUrl":"","userId":"11551777167550877658"}}},"source":["def label2Embedding(sentence):\n","  for word in sentence: \n","    if word in embedding.vocab:\n","      embed = embedding.get_vector(word)\n","      return embed\n","    else:\n","      print(\"This word is not in the vocabuylary: \", word, \"\\n\")"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"E1FdpbXN_0rz","executionInfo":{"status":"error","timestamp":1601596765257,"user_tz":180,"elapsed":30069,"user":{"displayName":"Carlos Reinheimer","photoUrl":"","userId":"11551777167550877658"}},"outputId":"47b08b30-b06f-4a29-c5d1-56da75a99258","colab":{"base_uri":"https://localhost:8080/","height":231}},"source":["#filename_txt = 'glove.6B.100d.txt'\n","\n","#with open(filename_txt, 'r+') as f:\n","#  content = f.read()\n","#  for i, l in enumerate(f):\n","#    pass\n","#  line = f'{i+1} 100'\n","#  f.seek(0, 0)\n","#  f.write(line.rstrip('\\r\\n') + '\\n' + content)\n","\n","#with open(filename_txt) as f:\n","#  for linha in range(10):\n","#    print(next(f))"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-3b73ee5359dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{i+1} 100'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\r\\n'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'i' is not defined"]}]},{"cell_type":"code","metadata":{"id":"BNIRQsPYVlTg"},"source":["# modelo = KeyedVectors.load_word2vec_format(filename_txt)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0v6HmrS9mi7N","executionInfo":{"status":"ok","timestamp":1603844886045,"user_tz":180,"elapsed":35848,"user":{"displayName":"Carlos Reinheimer","photoUrl":"","userId":"11551777167550877658"}},"outputId":"873e7684-10ee-4249-c81b-e45c562d4c7e","colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["!unzip -o '/content/drive/My Drive/Tarefinha de Data Science/Arquivos - Carlos/glove.6B.zip'"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Archive:  /content/drive/My Drive/Tarefinha de Data Science/Arquivos - Carlos/glove.6B.zip\n","  inflating: glove.6B.50d.txt        \n","  inflating: glove.6B.100d.txt       \n","  inflating: glove.6B.200d.txt       \n","  inflating: glove.6B.300d.txt       \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BaCQsoL1gwWE","executionInfo":{"status":"error","timestamp":1603844916328,"user_tz":180,"elapsed":685,"user":{"displayName":"Carlos Reinheimer","photoUrl":"","userId":"11551777167550877658"}},"outputId":"6624efa6-75ad-4892-9a23-d74294a9a186","colab":{"base_uri":"https://localhost:8080/","height":340}},"source":["from gensim.test.utils import datapath, get_tmpfile\n","from gensim.models import KeyedVectors\n","from gensim.scripts.glove2word2vec import glove2word2vec\n","\n","file_name = 'glove.6B.100d.txt'\n","glove_file = datapath(file_name)\n","# tmp_file = get_tmpfile(\"test_word2vec.txt\")\n","\n","# _ = glove2word2vec(glove_file)\n","\n","model = KeyedVectors.load_word2vec_format(glove_file)"],"execution_count":13,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-5d82495a6401>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# _ = glove2word2vec(glove_file)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1436\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1437\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1438\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# throws for invalid file format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0mignore_ext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mignore_extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, transport_params)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, ignore_ext, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/usr/local/lib/python3.6/dist-packages/gensim/test/test_data/glove.6B.100d.txt'"]}]},{"cell_type":"markdown","metadata":{"id":"rA0hBRL_OPzy"},"source":["----------------------------------------------------------------------------------------------------------------------------------"]},{"cell_type":"code","metadata":{"id":"L-LJvGSXOR25"},"source":["df = pd.read_csv(\"/content/drive/My Drive/Tarefinha de Data Science/Arquivos - Carlos/data.csv\", encoding = \"ISO-8859-1\")\n","df.tail()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3SLj5tPvZI8M"},"source":["Creating a vocabulary"]},{"cell_type":"code","metadata":{"id":"xOoGjKv3W1tQ"},"source":["dataset = df.copy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GNlORY0DXqKI"},"source":["dataset.drop(columns=[\"Sl no\", \"Search key\"]);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1i7iKErDXJgI"},"source":["dataset.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qUq4P84qHtmz"},"source":["dataset['Feeling'] = pd.Categorical(dataset['Feeling'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4N_clryjH1VU"},"source":["dataset['emotion_code'] = dataset['Feeling'].cat.codes\n","dataset.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cCwPAeX8GdmF"},"source":["## Cleaning texts"]},{"cell_type":"code","metadata":{"id":"9tnfKepfGgKr"},"source":["dataset[\"CleanText\"] = [tknzr.tokenize(word) for word in dataset[\"Tweets\"]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xxe37sZVGso7"},"source":["dataset[\"CleanText\"] = [cleanText(word) for word in dataset[\"CleanText\"]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fgd1E4ubHZjj"},"source":["dataset.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Td2Eb7UtP60k"},"source":["#Criando EL VOCABULÁRIO (Com ajuda do código do sor em: Introdução ao PyTorch: da Regressão Linear à NLP com word-embeddings)\n","vocab_set = set() # será usado para gerar o vocabulário principal\n","max_len_doc = 0   # vamos medir o maior comprimento das mensagens envidas, em número de tokens\n","sum_len_doc = 0   # vamos medir o valor médio de palavras (tokens) por mensagem\n","min_word_len = 3  # comprimento mínimo de um token (em número de caracteres) para entrar no vocabulário \n","tokens_list = []  # salvar a lista de tokens\n","\n","for doc in dataset['CleanText']: # para cada documento do dataset\n","  #Pegando a palavra apenas se ela é maior que o comprimento mínimo\n","  for word in doc: \n","    if len(word)>=min_word_len:\n","      tokens_list.append(word)\n","\n","  # uso da função set: cria um conjunto dos elementos únicos da lista\n","  tokens_set = set(tokens_list)\n","  vocab_set = set.union(vocab_set, tokens_set) # adiciona elementos únicos que ainda não pertencem ao conjunto do vocabulário\n","\n","  sum_len_doc += len(word)\n","  if len(word) > max_len_doc:\n","    max_len_doc=len(word)\n","\n","print(f'Tamanho total do vocabulário: V={len(vocab_set)}')\n","print(f'Número de palavras do texto mais longo: {max_len_doc}')\n","print(f'Média de palavras por texto: {sum_len_doc/len(df):3.4f}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VkdQIPrS_7IS"},"source":["--------------------------------------------------------------------------------------"]},{"cell_type":"markdown","metadata":{"id":"TCEMDP9HADXC"},"source":["## Creating vocabulary based on trained word embeddding"]},{"cell_type":"code","metadata":{"id":"kIhMThQX_7si"},"source":["word2idx = dict({})        # inicializa o dicionário\n","word2idx['<OOV>'] = 0      # índice da tag \"out of vocabulary\" é 0\n","word2idx['<PAD>'] = 1      # índice da tag \"padding token\" é 1\n","\n","for i, v in enumerate(sorted(vocab_set),start=2): # enumera o vocabulário em ordem alfabética, a partir do índice 2\n","  word2idx[v] = label2Embedding(v)\n","\n","# testando a conversão \"word to index\" com o dicionário:\n","print(f'index for \"<PAD>\": {word2idx[\"<PAD>\"]}')\n","print(f'index for \"action\": {word2idx[\"action\"]}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eawW6n9o_8Kb"},"source":["---------------------------------------------------------------------------------------"]},{"cell_type":"code","metadata":{"id":"wVgn3DYpJIZF"},"source":["#word2idx = dict({})        # inicializa o dicionário\n","#word2idx['<OOV>'] = 0      # índice da tag \"out of vocabulary\" é 0\n","#word2idx['<PAD>'] = 1      # índice da tag \"padding token\" é 1\n","\n","#for i, v in enumerate(sorted(vocab_set),start=2): # enumera o vocabulário em ordem alfabética, a partir do índice 2\n","#  word2idx[v] = i\n","\n","# testando a conversão \"word to index\" com o dicionário:\n","#print(f'index for \"<PAD>\": {word2idx[\"<PAD>\"]}')\n","#print(f'index for \"action\": {word2idx[\"action\"]}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3jIDIUf0JOQD"},"source":["idx2word = list(word2idx.keys()) # apenas transforma as chaves (palavras ordenadas) do dicionário word2idx em uma lista\n","\n","# testando a conversão \"index to word\":\n","print(f'word for index 0:    {idx2word[0]}')\n","print(f'word for index 100\": {idx2word[100]}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"43qiOkZEJXzB"},"source":["Coding the texts"]},{"cell_type":"code","metadata":{"id":"KiZFBVtlJx7k"},"source":["max_len = 25         # comprimento máximo da mensagem (em número de palavras)\n","encoded_docs = []    # inicializa a lista de documentos codificados\n","\n","for doc in dataset['CleanText']: # para cada texto\n","  encoded_d = [word2idx.get(t,word2idx['<OOV>']) for t in doc]    # codifica o documento usando o dicionário word2idx\n","  encoded_d += [word2idx['<PAD>']]*max(0, max_len-len(encoded_d))    # adiciona o padding, se necessário\n","  \n","  encoded_docs.append(encoded_d[:max_len])                           # trunca o documento e salva na lista de documentos codificados\n","\n","len(encoded_docs)  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LRqo8v9KKQae"},"source":["dataset['CleanText'] = encoded_docs\n","dataset.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L_5jq-gMLEq9"},"source":["Splitting into Train and Test"]},{"cell_type":"code","metadata":{"id":"UNjxLiLlK9nn"},"source":["X = np.vstack(dataset['CleanText'].apply(lambda x: np.array(x)))\n","Y = np.array(dataset['emotion_code']).reshape(-1,1)\n","X.shape, X[0].shape, Y.shape, Y[0].shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G21_Y4UxLER0"},"source":["train_size = 0.8    # percentual de exemplos para o treino\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(X,Y,                       # dataset para ser dividido, entrada X e saída Y\n","                                                    train_size=train_size,     # percentual resevado para o treinamento\n","                                                    stratify=Y,                # estratificação para manter a distribuição dos rótulos igual entre treino e teste\n","                                                    shuffle=True)              # embaralhar os exemplos aleatoriamente"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qT0hVbqYyEbH"},"source":["1. **Seguir o tutorial pra ter acesso aos embedding de cada palavra**\n","\n","2. Converter as palavras para o próprio embedding durante o treinamento (1 frase por vez) -- criar funçao pra tal\n","  - usar  embedding.get_vector(palavra)\n","\n","3. pyTortch dataLoader - batchSize --> https://pytorch.org/docs/stable/data.html\n"]},{"cell_type":"code","metadata":{"id":"3qC2D1U3JKxv"},"source":["#words = [\"draw\", \"to\", \"the\", \"ground\"]\n","#teste = [label2Embedding(word) for word in words]\n","#teste"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qf8exmX6MXRs"},"source":["Creating model"]},{"cell_type":"code","metadata":{"id":"F9t2TFGwKZS2"},"source":["class Torch_Mean_Layer(nn.Module):\n","  '''Camada personalizada: calcula a média do tensor dentrada sobre a dimensão 1 (colunas).\n","     Retorna um vetor linha, onde cada elemento é a média dos elementos da coluna correspondente do tensor de entrada.\n","  '''\n","  def forward(self, x, dim=1):\n","    print(\"-----------------------\",x)\n","    x = torch.mean(x, dim=dim, keepdims=True)\n","    return x\n","\n","class mood_classifier(nn.Module):\n","  '''Modelo classificador de emoções\n","  '''\n","\n","  # ----------------------------------------------#\n","  # Método construtor\n","  def __init__(self, vocab_size, dim_embed, n_units): \n","    super().__init__()  \n","\n","    embedding_seq = [] # \n","    ann_seq       = [] # \n","    soft_seq      = []\n","\n","    #---------------------------------------------------------------#\n","    # Embedding step: sequência de operações para converter X --> h\n","    embedding_seq.append(Torch_Mean_Layer())\n","    #---------------------------------------------------------------#\n","\n","    #--------------------------------------------------------------------------#\n","    # ANN: Rede Neural Artifical Tradicional, com regressão logística na saída\n","    ann_seq.append(nn.Linear(dim_embed, n_units))\n","    ann_seq.append(nn.ReLU(inplace=True))\n","    ann_seq.append(nn.Linear(n_units, 6))\n","    \n","    #--------------------------------------------------------------------------#\n","    # Softmax :)\n","    soft_seq.append(nn.LogSoftmax(dim=1))\n","\n","    #--------------------------------------------------------------------------#\n","\n","    #--------------------------------------------------------------------------#\n","    # \"merge\" de todas as camamadas em uma layer sequencial \n","    # (uma sequência para cada etapa)\n","    self.embedding = nn.Sequential(*embedding_seq)     # etapa de embedding \n","    self.ann       = nn.Sequential(*ann_seq)           # etapa ANN\n","    self.soft      = nn.Sequential(*soft_seq)\n","    #--------------------------------------------------------------------------#\n","\n","\n","  def forward(self, x): \n","    '''Processamento realizado ao chamar y=modelo(x)\n","    '''\n","    x = self.embedding(x)  # aplica a etapa de embedding\n","    x = self.ann(x)        # passa o embedding médio pelas camadas da ANN\n","    x = x.view(-1,6)\n","    x = self.soft(x)\n","    return x  #Adcionar o softmax"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ss8SyoiBMaY8"},"source":["Training model"]},{"cell_type":"code","metadata":{"id":"w06ib3KfKgKU"},"source":["import tensorflow as tf\n","def train_loop(model, data, max_epochs = 1000, print_iters = 5):\n","  X_train, Y_train = data\n","  losses = []\n","  accs = []\n","  for i in range(max_epochs): # para cada época\n","\n","      #-----------------------------------#\n","      # INÍCIO DO WORKFLOW DO TREINAMENTO #\n","      # \n","      # Add mistura\n","\n","      Y_pred = model.forward(X_train)         # apresente os dados de entrada para o modelo, e obtenha a previsão    \n","      loss = criterion(Y_pred.view(-1, 6), Y_train.view(-1))       # calcule a perda (o custo, o erro)\n","      optimizer.zero_grad()                   # inicialize os gradientes\n","      loss.backward()                         # backpropagation sobre a perda atual (cálculo dos novos gradientes) \n","      optimizer.step()                        # atualização dos parâmetros da rede utilizando a regra do otimizador escolhido\n","      # FIM DO WORKFLOW DO TREINAMENTO    #\n","      #-----------------------------------#\n","\n","      # ------ Bloco Opcional ------ #\n","      # Salvando métricas\n","      losses.append(loss)                     # salvando a perda atual\n","      acc = calc_accuracy(Y_pred, Y_train)     # calcula a taxa de acerto atual\n","      accs.append(acc)\n","      \n","      # Imprimindo resultados parciais\n","      if i % print_iters ==0: # a cada 10 iterações\n","        print(f'epoch: {i:2}  loss: {loss.item():10.8f}') \n","      #-----------------------------------#\n","\n","  #----------------------------------------------------------------------------# \n","  print('\\n# Finished training!')\n","  print(f'# --> epoch: {i}  \\n# --> initial loss: {losses[0]:10.8f} ,  \\n# --> accuracy: {acc:2.8f} , \\n# --> final loss: {losses[-1]:10.8f}')\n","  \n","  # retornando resultados\n","  return model, losses, accs\n","\n","# Redefinindo cálculo da taxa de acerto \n","def calc_accuracy(y_pred, y_true):\n","  ''' Helper function para calcular a taxa de acerto deste exemplo.\n","  '''\n","  y_pred = torch.argmax(y_pred, dim=1)\n","  y_pred = y_pred.float()\n","  y_true = torch.squeeze(y_true) # tentar rexplicar dps\n","  y_pred = torch.squeeze(y_pred)\n","  num_hits  = torch.sum(y_pred==y_true).numpy()\n","  num_total =  float(y_true.numel())\n","  acc=  num_hits/num_total\n","  return acc"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cXJdUs0TNco9"},"source":["Now, we're convert everything to Tensors and instantiate our loss function"]},{"cell_type":"code","metadata":{"id":"1JfCBcGxQL2K"},"source":["x_train = np.vstack(X_train)\n","y_train = np.array(Y_train).reshape(-1,1)\n","x_train.shape, x_train[0].shape, y_train.shape, y_train[0].shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2wf3yRuXNizi"},"source":["import tensorflow as tf\n","\n","#data_train = (tf.constant(x_train, dtype=tf.float64), tf.constant(y_train, dtype=tf.float64))\n","data_train = (torch.float32(X_train), torch.float32(Y_train))\n","\n","Model = mood_classifier(vocab_size=len(word2idx), dim_embed=100, n_units=100)\n","print(Model)\n","\n","criterion = nn.NLLLoss() # cross entropy loss\n","optimizer = torch.optim.Adam(Model.parameters(), lr = 0.01) \n","\n","Model, losses, accs = train_loop(Model, data_train, max_epochs=330, print_iters=1) # note que o modelo é sobrescrito pela saída treinada"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OFfxQbKeYcRY"},"source":["--------------------------------------------------------------------------------------------------"]},{"cell_type":"code","metadata":{"id":"nzagyAB2VGKR"},"source":["X_train = torch.LongTensor(X_train)\n","Y_train = torch.LongTensor(Y_train)\n","X_test = np.vstack(X_test)\n","X_test = torch.LongTensor(X_test)\n","Y_test = np.array(Y_test).reshape(-1,1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IC425RVHVCw5"},"source":["Y_pred = torch.exp(Model.forward(X_test))         "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fm8CH5b8U_ua"},"source":["Y_pred = torch.argmax(Y_pred, dim=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QpRsLdh5n0yO"},"source":["dataset[\"Feeling\"].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fuFI3C5woXnt"},"source":["dataset[\"emotion_code\"].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CUPktCTpQQ9e"},"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix\n","\n","sns.set(font_scale=1.5)\n","\n","matriz_de_confusao = confusion_matrix(Y_pred, Y_test)\n","\n","#criando list com as emoções\n","emotion_class = ['Angry','Disgust','Fear','Happy','Sad','Surprise']\n","\n","df_matriz_de_confusao = pd.DataFrame(matriz_de_confusao, emotion_class, emotion_class)\n","\n","# confusion matrix daora TEM Q MELHORAR\n","def plot_conf_mat(y_test, y_preds, norm='true'):\n","   fig, ax = plt.subplots(figsize=(8, 6))\n","   ax = sns.heatmap(df_matriz_de_confusao,\n","                   annot=True,\n","                    fmt=\"d\",\n","                    cmap=\"YlOrRd\")\n","   plt.xlabel(\"Resultado previsto\")\n","   plt.ylabel(\"Resultado real\")\n","\n","\n","plot_conf_mat(Y_test, Y_pred)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BCUY0m-omsTs"},"source":["NICE GRAPHICS"]},{"cell_type":"code","metadata":{"id":"1c72jSwDmvKE"},"source":["def plot_loss_and_accuracy(losses, accs):\n","\n","  fig, ax_tuple = plt.subplots(1, 2, figsize=(16,6))\n","  fig.suptitle('Loss and accuracy')\n","\n","  for i, (y_label, y_values) in enumerate(zip(['CE loss','Accuracy'],[losses, accs])):\n","    ax_tuple[i].plot(range(len(y_values)),  y_values, label='train')\n","    ax_tuple[i].set_xlabel('epochs')\n","    ax_tuple[i].set_ylabel(y_label)\n","    ax_tuple[i].legend()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"frcaHCLHmw2R"},"source":["plot_loss_and_accuracy(losses, accs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Alze2r2iVpLg"},"source":["Instantiating another model"]},{"cell_type":"code","metadata":{"id":"eEAv5Vevm8Dw"},"source":["#model = torch.load(\"/content/drive/My Drive/Análise de Sentimentos/Projeto para 08-09/Models/model.pth\")\n","#model.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pWyLsC_8VtnG"},"source":["#data_train = (torch.LongTensor(x_train), torch.LongTensor(y_train))\n","\n","#model = model(model, vocab_size=len(word2idx), dim_embed=100, n_units=100)\n","\n","#criterion = nn.NLLLoss() # cross entropy loss\n","#optimizer = torch.optim.Adam(Model.parameters(), lr = 0.01) \n","\n","#model, losses, accs = train_loop(model, data_train, max_epochs=100, print_iters=1) # note que o modelo é sobrescrito pela saída treinada"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3bgIshr92fw7"},"source":["https://keras.io/examples/nlp/pretrained_word_embeddings/"]}]}