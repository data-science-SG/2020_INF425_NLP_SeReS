{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Multiclass Classification Problem with SoftMax and Bokeh.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"OAJa_FwAkiqp"},"source":["# Movie genre multiclass classification problem using softmax and bookeh\n","\n","Firsts things firsts, this notebook will use the data distributed by Kaggle: https://www.kaggle.com/shainy/twitter-reviews-for-emotion-analysis\n","<br />\n","Resuming, it's about movie genre\n","<br /><br />\n","In this notebook, we will try to predict the movie genre using the **softmax function** and after, plotting using a bokeh graph"]},{"cell_type":"markdown","metadata":{"id":"8fQKQQxZmDlC"},"source":["----------------------------------------------------------------------------------------------------------------------------------------------------------------"]},{"cell_type":"markdown","metadata":{"id":"2Os3Kj6MmGRU"},"source":["## Beginning of the work\n","\n","To begin, we will mount the connection with drive so we can acess the csv"]},{"cell_type":"code","metadata":{"id":"7RyJhUqGimUc","executionInfo":{"status":"ok","timestamp":1603843425848,"user_tz":180,"elapsed":40027,"user":{"displayName":"Carlos Reinheimer","photoUrl":"","userId":"11551777167550877658"}},"outputId":"b0f9ec37-bc29-45bf-86fb-f4a9a08ba131","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mndyedcdmYew"},"source":["Besides, we will import and use **sys**, so we can import the functions that we are going to use of a **.py** file"]},{"cell_type":"code","metadata":{"id":"qlbE-71empwf","executionInfo":{"status":"ok","timestamp":1603843425849,"user_tz":180,"elapsed":40018,"user":{"displayName":"Carlos Reinheimer","photoUrl":"","userId":"11551777167550877658"}}},"source":["import sys\n","sys.path.append('/content/drive/My Drive/Tarefinha de Data Science/Arquivos - Carlos')"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2ToRyRnNkhQM"},"source":["## Imports\n","\n","Before starting to use the data, we will import all the things we need\n"]},{"cell_type":"code","metadata":{"id":"5a_PZcxTnFpO","executionInfo":{"status":"ok","timestamp":1603843451849,"user_tz":180,"elapsed":7311,"user":{"displayName":"Carlos Reinheimer","photoUrl":"","userId":"11551777167550877658"}},"outputId":"5ee8a8e2-ce7f-407c-cbbc-7d08d9373e8e","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# dataset\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline \n","# pd.set_option('display.max_colwidth', -1)\n","\n","# my functions\n","# import UtilsCarlos\n","# from UtilsCarlos import  criaDicio, criaVetor, fit_and_score, plot_conf_mat, convert\n","\n","# nltk\n","import nltk\n","from nltk.tokenize import TweetTokenizer\n","import re\n","from nltk import FreqDist\n","from nltk.tokenize import TweetTokenizer\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords \n","tknzr = TweetTokenizer()\n","stemmer = PorterStemmer()\n","nltk.download('punkt')\n","\n","# modelos\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn import tree\n","\n","# evaluations\n","from sklearn.model_selection import train_test_split, cross_val_score\n","from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n","from sklearn.metrics import confusion_matrix, classification_report\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","from sklearn.metrics import plot_roc_curve\n","from sklearn.model_selection import cross_val_score\n","\n","# others\n","import json\n","import string\n","\n","# PyTorch\n","#import torch\n","#from torch import nn"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\carlo\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"code","metadata":{"id":"MQAebXvbOZia","executionInfo":{"status":"ok","timestamp":1603843456688,"user_tz":180,"elapsed":724,"user":{"displayName":"Carlos Reinheimer","photoUrl":"","userId":"11551777167550877658"}},"outputId":"8f7f9e8b-aabd-4214-a9d9-e142e19ee8df","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["nltk.download('stopwords')\n","stopwords = nltk.corpus.stopwords.words(\"english\")\n","token_espaço = nltk.tokenize.WhitespaceTokenizer()"],"execution_count":4,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VQMaxZrvNbt9"},"source":["---------------------------------------------------------------------------------------------------------"]},{"cell_type":"code","metadata":{"id":"XHLCEe5iONyu","executionInfo":{"status":"ok","timestamp":1603843459040,"user_tz":180,"elapsed":882,"user":{"displayName":"Carlos Reinheimer","photoUrl":"","userId":"11551777167550877658"}}},"source":["# função de limpar texto\n","def cleanText(words, stem=False):\n","  \"\"\"\n","    Esta função recebe um text e retorna o mesmo, já tratado com stopwords & punctuation\n","  \"\"\"\n","  newWords = list()\n","  pontuacao = string,\n","  for word in words:\n","    word = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n","                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', word)\n","    words = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", word)\n","    if len(word) > 0 and words not in string.punctuation and word.lower() not in stopwords and word.lower != \"<br />\":\n","      if stem:\n","        word = stemmer.stem(word.lower())\n","        newWords.append(word)\n","      else:\n","        newWords.append(word.lower())\n","\n","  return newWords"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"QMEdrkjdOOmy","executionInfo":{"status":"ok","timestamp":1603843459042,"user_tz":180,"elapsed":879,"user":{"displayName":"Carlos Reinheimer","photoUrl":"","userId":"11551777167550877658"}}},"source":["# confusion matrix daora\n","def plot_conf_mat(y_test, y_preds, norm=\"false\"):\n","    fig, ax = plt.subplots(figsize=(3, 3))\n","    ax = sns.heatmap(confusion_matrix(y_test, y_preds, normalize=norm),\n","                    annot=True,\n","                    cbar=False)\n","    plt.xlabel(\"True label\")\n","    plt.ylabel(\"Predicted label\")"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"T7kT0ZfyYt0s","executionInfo":{"status":"ok","timestamp":1603843460103,"user_tz":180,"elapsed":619,"user":{"displayName":"Carlos Reinheimer","photoUrl":"","userId":"11551777167550877658"}}},"source":["def plot_loss_and_accuracy(losses, accs):\n","\n","  fig, ax_tuple = plt.subplots(1, 2, figsize=(16,6))\n","  fig.suptitle('Loss and accuracy')\n","\n","  for i, (y_label, y_values) in enumerate(zip(['BCE loss','Accuracy'],[losses, accs])):\n","    ax_tuple[i].plot(range(len(y_values)),  y_values, label='train')\n","    ax_tuple[i].set_xlabel('epochs')\n","    ax_tuple[i].set_ylabel(y_label)\n","    ax_tuple[i].legend()"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"iAKxRC_WV9Ox","executionInfo":{"status":"ok","timestamp":1603843460403,"user_tz":180,"elapsed":784,"user":{"displayName":"Carlos Reinheimer","photoUrl":"","userId":"11551777167550877658"}}},"source":["def label2Embedding(sentence):\n","  for word in sentence: \n","    if word in embedding.vocab:\n","      embed = embedding.get_vector(word)\n","      return embed\n","    else:\n","      print(\"This word is not in the vocabuylary: \", word, \"\\n\")"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"E1FdpbXN_0rz","executionInfo":{"status":"error","timestamp":1601596765257,"user_tz":180,"elapsed":30069,"user":{"displayName":"Carlos Reinheimer","photoUrl":"","userId":"11551777167550877658"}},"outputId":"47b08b30-b06f-4a29-c5d1-56da75a99258","colab":{"base_uri":"https://localhost:8080/","height":231}},"source":["filename_txt = '../data/glove.6B.50d.txt'\n","\n","with open(filename_txt, 'r+', encoding=\"utf8\") as f:\n","  content = f.read()\n","  for i, l in enumerate(f):\n","    pass\n","    line = f'{i+1} 100'\n","    f.seek(0, 0)\n","    f.write(line.rstrip('\\r\\n') + '\\n' + content)\n","\n","with open(filename_txt) as f:\n","  for linha in range(10):\n","    print(next(f))"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 -0.6566 0.27843 -0.14767 -0.55677 0.14658 -0.0095095 0.011658 0.10204 -0.12792 -0.8443 -0.12181 -0.016801 -0.33279 -0.1552 -0.23131 -0.19181 -1.8823 -0.76746 0.099051 -0.42125 -0.19526 4.0071 -0.18594 -0.52287 -0.31681 0.00059213 0.0074449 0.17778 -0.15897 0.012041 -0.054223 -0.29871 -0.15749 -0.34758 -0.045637 -0.44251 0.18785 0.0027849 -0.18411 -0.11514 -0.78581\n\n, 0.013441 0.23682 -0.16899 0.40951 0.63812 0.47709 -0.42852 -0.55641 -0.364 -0.23938 0.13001 -0.063734 -0.39575 -0.48162 0.23291 0.090201 -0.13324 0.078639 -0.41634 -0.15428 0.10068 0.48891 0.31226 -0.1252 -0.037512 -1.5179 0.12612 -0.02442 -0.042961 -0.28351 3.5416 -0.11956 -0.014533 -0.1499 0.21864 -0.33412 -0.13872 0.31806 0.70358 0.44858 -0.080262 0.63003 0.32111 -0.46765 0.22786 0.36034 -0.37818 -0.56657 0.044691 0.30392\n\n. 0.15164 0.30177 -0.16763 0.17684 0.31719 0.33973 -0.43478 -0.31086 -0.44999 -0.29486 0.16608 0.11963 -0.41328 -0.42353 0.59868 0.28825 -0.11547 -0.041848 -0.67989 -0.25063 0.18472 0.086876 0.46582 0.015035 0.043474 -1.4671 -0.30384 -0.023441 0.30589 -0.21785 3.746 0.0042284 -0.18436 -0.46209 0.098329 -0.11907 0.23919 0.1161 0.41705 0.056763 -6.3681e-05 0.068987 0.087939 -0.10285 -0.13931 0.22314 -0.080803 -0.35652 0.016413 0.10216\n\nof 0.70853 0.57088 -0.4716 0.18048 0.54449 0.72603 0.18157 -0.52393 0.10381 -0.17566 0.078852 -0.36216 -0.11829 -0.83336 0.11917 -0.16605 0.061555 -0.012719 -0.56623 0.013616 0.22851 -0.14396 -0.067549 -0.38157 -0.23698 -1.7037 -0.86692 -0.26704 -0.2589 0.1767 3.8676 -0.1613 -0.13273 -0.68881 0.18444 0.0052464 -0.33874 -0.078956 0.24185 0.36576 -0.34727 0.28483 0.075693 -0.062178 -0.38988 0.22902 -0.21617 -0.22562 -0.093918 -0.80375\n\nto 0.68047 -0.039263 0.30186 -0.17792 0.42962 0.032246 -0.41376 0.13228 -0.29847 -0.085253 0.17118 0.22419 -0.10046 -0.43653 0.33418 0.67846 0.057204 -0.34448 -0.42785 -0.43275 0.55963 0.10032 0.18677 -0.26854 0.037334 -2.0932 0.22171 -0.39868 0.20912 -0.55725 3.8826 0.47466 -0.95658 -0.37788 0.20869 -0.32752 0.12751 0.088359 0.16351 -0.21634 -0.094375 0.018324 0.21048 -0.03088 -0.19722 0.082279 -0.09434 -0.073297 -0.064699 -0.26044\n\nand 0.26818 0.14346 -0.27877 0.016257 0.11384 0.69923 -0.51332 -0.47368 -0.33075 -0.13834 0.2702 0.30938 -0.45012 -0.4127 -0.09932 0.038085 0.029749 0.10076 -0.25058 -0.51818 0.34558 0.44922 0.48791 -0.080866 -0.10121 -1.3777 -0.10866 -0.23201 0.012839 -0.46508 3.8463 0.31362 0.13643 -0.52244 0.3302 0.33707 -0.35601 0.32431 0.12041 0.3512 -0.069043 0.36885 0.25168 -0.24517 0.25381 0.1367 -0.31178 -0.6321 -0.25028 -0.38097\n\nin 0.33042 0.24995 -0.60874 0.10923 0.036372 0.151 -0.55083 -0.074239 -0.092307 -0.32821 0.09598 -0.82269 -0.36717 -0.67009 0.42909 0.016496 -0.23573 0.12864 -1.0953 0.43334 0.57067 -0.1036 0.20422 0.078308 -0.42795 -1.7984 -0.27865 0.11954 -0.12689 0.031744 3.8631 -0.17786 -0.082434 -0.62698 0.26497 -0.057185 -0.073521 0.46103 0.30862 0.12498 -0.48609 -0.0080272 0.031184 -0.36576 -0.42699 0.42164 -0.11666 -0.50703 -0.027273 -0.53285\n\na 0.21705 0.46515 -0.46757 0.10082 1.0135 0.74845 -0.53104 -0.26256 0.16812 0.13182 -0.24909 -0.44185 -0.21739 0.51004 0.13448 -0.43141 -0.03123 0.20674 -0.78138 -0.20148 -0.097401 0.16088 -0.61836 -0.18504 -0.12461 -2.2526 -0.22321 0.5043 0.32257 0.15313 3.9636 -0.71365 -0.67012 0.28388 0.21738 0.14433 0.25926 0.23434 0.4274 -0.44451 0.13813 0.36973 -0.64289 0.024142 -0.039315 -0.26037 0.12017 -0.043782 0.41013 0.1796\n\n\" 0.25769 0.45629 -0.76974 -0.37679 0.59272 -0.063527 0.20545 -0.57385 -0.29009 -0.13662 0.32728 1.4719 -0.73681 -0.12036 0.71354 -0.46098 0.65248 0.48887 -0.51558 0.039951 -0.34307 -0.014087 0.86488 0.3546 0.7999 -1.4995 -1.8153 0.41128 0.23921 -0.43139 3.6623 -0.79834 -0.54538 0.16943 -0.82017 -0.3461 0.69495 -1.2256 -0.17992 -0.057474 0.030498 -0.39543 -0.38515 -1.0002 0.087599 -0.31009 -0.34677 -0.31438 0.75004 0.97065\n\n's 0.23727 0.40478 -0.20547 0.58805 0.65533 0.32867 -0.81964 -0.23236 0.27428 0.24265 0.054992 0.16296 -1.2555 -0.086437 0.44536 0.096561 -0.16519 0.058378 -0.38598 0.086977 0.0033869 0.55095 -0.77697 -0.62096 0.092948 -2.5685 -0.67739 0.10151 -0.48643 -0.057805 3.1859 -0.017554 -0.16138 0.055486 -0.25885 -0.33938 -0.19928 0.26049 0.10478 -0.55934 -0.12342 0.65961 -0.51802 -0.82995 -0.082739 0.28155 -0.423 -0.27378 -0.007901 -0.030231\n\n"]}]},{"cell_type":"code","metadata":{"id":"BNIRQsPYVlTg"},"source":["# gensin\n","from gensim.models import KeyedVectors\n","\n","modelo = KeyedVectors.load_word2vec_format(filename_txt)"],"execution_count":3,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'gensim'","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[1;32m<ipython-input-3-30b5b6a75dd2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# gensin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodelo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename_txt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"]}]},{"cell_type":"code","metadata":{"id":"0v6HmrS9mi7N","executionInfo":{"status":"ok","timestamp":1603844886045,"user_tz":180,"elapsed":35848,"user":{"displayName":"Carlos Reinheimer","photoUrl":"","userId":"11551777167550877658"}},"outputId":"873e7684-10ee-4249-c81b-e45c562d4c7e","colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["!unzip -o '/content/drive/My Drive/Tarefinha de Data Science/Arquivos - Carlos/glove.6B.zip'"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Archive:  /content/drive/My Drive/Tarefinha de Data Science/Arquivos - Carlos/glove.6B.zip\n","  inflating: glove.6B.50d.txt        \n","  inflating: glove.6B.100d.txt       \n","  inflating: glove.6B.200d.txt       \n","  inflating: glove.6B.300d.txt       \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BaCQsoL1gwWE","executionInfo":{"status":"error","timestamp":1603844916328,"user_tz":180,"elapsed":685,"user":{"displayName":"Carlos Reinheimer","photoUrl":"","userId":"11551777167550877658"}},"outputId":"6624efa6-75ad-4892-9a23-d74294a9a186","colab":{"base_uri":"https://localhost:8080/","height":340}},"source":["from gensim.test.utils import datapath, get_tmpfile\n","from gensim.models import KeyedVectors\n","from gensim.scripts.glove2word2vec import glove2word2vec\n","\n","file_name = 'glove.6B.100d.txt'\n","glove_file = datapath(file_name)\n","# tmp_file = get_tmpfile(\"test_word2vec.txt\")\n","\n","# _ = glove2word2vec(glove_file)\n","\n","model = KeyedVectors.load_word2vec_format(glove_file)"],"execution_count":13,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-5d82495a6401>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# _ = glove2word2vec(glove_file)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1436\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1437\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1438\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# throws for invalid file format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0mignore_ext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mignore_extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, transport_params)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, ignore_ext, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/usr/local/lib/python3.6/dist-packages/gensim/test/test_data/glove.6B.100d.txt'"]}]},{"cell_type":"markdown","metadata":{"id":"rA0hBRL_OPzy"},"source":["----------------------------------------------------------------------------------------------------------------------------------"]},{"cell_type":"code","metadata":{"id":"L-LJvGSXOR25"},"source":["df = pd.read_csv(\"/content/drive/My Drive/Tarefinha de Data Science/Arquivos - Carlos/data.csv\", encoding = \"ISO-8859-1\")\n","df.tail()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3SLj5tPvZI8M"},"source":["Creating a vocabulary"]},{"cell_type":"code","metadata":{"id":"xOoGjKv3W1tQ"},"source":["dataset = df.copy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GNlORY0DXqKI"},"source":["dataset.drop(columns=[\"Sl no\", \"Search key\"]);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1i7iKErDXJgI"},"source":["dataset.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qUq4P84qHtmz"},"source":["dataset['Feeling'] = pd.Categorical(dataset['Feeling'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4N_clryjH1VU"},"source":["dataset['emotion_code'] = dataset['Feeling'].cat.codes\n","dataset.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cCwPAeX8GdmF"},"source":["## Cleaning texts"]},{"cell_type":"code","metadata":{"id":"9tnfKepfGgKr"},"source":["dataset[\"CleanText\"] = [tknzr.tokenize(word) for word in dataset[\"Tweets\"]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xxe37sZVGso7"},"source":["dataset[\"CleanText\"] = [cleanText(word) for word in dataset[\"CleanText\"]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fgd1E4ubHZjj"},"source":["dataset.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Td2Eb7UtP60k"},"source":["#Criando EL VOCABULÁRIO (Com ajuda do código do sor em: Introdução ao PyTorch: da Regressão Linear à NLP com word-embeddings)\n","vocab_set = set() # será usado para gerar o vocabulário principal\n","max_len_doc = 0   # vamos medir o maior comprimento das mensagens envidas, em número de tokens\n","sum_len_doc = 0   # vamos medir o valor médio de palavras (tokens) por mensagem\n","min_word_len = 3  # comprimento mínimo de um token (em número de caracteres) para entrar no vocabulário \n","tokens_list = []  # salvar a lista de tokens\n","\n","for doc in dataset['CleanText']: # para cada documento do dataset\n","  #Pegando a palavra apenas se ela é maior que o comprimento mínimo\n","  for word in doc: \n","    if len(word)>=min_word_len:\n","      tokens_list.append(word)\n","\n","  # uso da função set: cria um conjunto dos elementos únicos da lista\n","  tokens_set = set(tokens_list)\n","  vocab_set = set.union(vocab_set, tokens_set) # adiciona elementos únicos que ainda não pertencem ao conjunto do vocabulário\n","\n","  sum_len_doc += len(word)\n","  if len(word) > max_len_doc:\n","    max_len_doc=len(word)\n","\n","print(f'Tamanho total do vocabulário: V={len(vocab_set)}')\n","print(f'Número de palavras do texto mais longo: {max_len_doc}')\n","print(f'Média de palavras por texto: {sum_len_doc/len(df):3.4f}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VkdQIPrS_7IS"},"source":["--------------------------------------------------------------------------------------"]},{"cell_type":"markdown","metadata":{"id":"TCEMDP9HADXC"},"source":["## Creating vocabulary based on trained word embeddding"]},{"cell_type":"code","metadata":{"id":"kIhMThQX_7si"},"source":["word2idx = dict({})        # inicializa o dicionário\n","word2idx['<OOV>'] = 0      # índice da tag \"out of vocabulary\" é 0\n","word2idx['<PAD>'] = 1      # índice da tag \"padding token\" é 1\n","\n","for i, v in enumerate(sorted(vocab_set),start=2): # enumera o vocabulário em ordem alfabética, a partir do índice 2\n","  word2idx[v] = label2Embedding(v)\n","\n","# testando a conversão \"word to index\" com o dicionário:\n","print(f'index for \"<PAD>\": {word2idx[\"<PAD>\"]}')\n","print(f'index for \"action\": {word2idx[\"action\"]}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eawW6n9o_8Kb"},"source":["---------------------------------------------------------------------------------------"]},{"cell_type":"code","metadata":{"id":"wVgn3DYpJIZF"},"source":["#word2idx = dict({})        # inicializa o dicionário\n","#word2idx['<OOV>'] = 0      # índice da tag \"out of vocabulary\" é 0\n","#word2idx['<PAD>'] = 1      # índice da tag \"padding token\" é 1\n","\n","#for i, v in enumerate(sorted(vocab_set),start=2): # enumera o vocabulário em ordem alfabética, a partir do índice 2\n","#  word2idx[v] = i\n","\n","# testando a conversão \"word to index\" com o dicionário:\n","#print(f'index for \"<PAD>\": {word2idx[\"<PAD>\"]}')\n","#print(f'index for \"action\": {word2idx[\"action\"]}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3jIDIUf0JOQD"},"source":["idx2word = list(word2idx.keys()) # apenas transforma as chaves (palavras ordenadas) do dicionário word2idx em uma lista\n","\n","# testando a conversão \"index to word\":\n","print(f'word for index 0:    {idx2word[0]}')\n","print(f'word for index 100\": {idx2word[100]}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"43qiOkZEJXzB"},"source":["Coding the texts"]},{"cell_type":"code","metadata":{"id":"KiZFBVtlJx7k"},"source":["max_len = 25         # comprimento máximo da mensagem (em número de palavras)\n","encoded_docs = []    # inicializa a lista de documentos codificados\n","\n","for doc in dataset['CleanText']: # para cada texto\n","  encoded_d = [word2idx.get(t,word2idx['<OOV>']) for t in doc]    # codifica o documento usando o dicionário word2idx\n","  encoded_d += [word2idx['<PAD>']]*max(0, max_len-len(encoded_d))    # adiciona o padding, se necessário\n","  \n","  encoded_docs.append(encoded_d[:max_len])                           # trunca o documento e salva na lista de documentos codificados\n","\n","len(encoded_docs)  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LRqo8v9KKQae"},"source":["dataset['CleanText'] = encoded_docs\n","dataset.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L_5jq-gMLEq9"},"source":["Splitting into Train and Test"]},{"cell_type":"code","metadata":{"id":"UNjxLiLlK9nn"},"source":["X = np.vstack(dataset['CleanText'].apply(lambda x: np.array(x)))\n","Y = np.array(dataset['emotion_code']).reshape(-1,1)\n","X.shape, X[0].shape, Y.shape, Y[0].shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G21_Y4UxLER0"},"source":["train_size = 0.8    # percentual de exemplos para o treino\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(X,Y,                       # dataset para ser dividido, entrada X e saída Y\n","                                                    train_size=train_size,     # percentual resevado para o treinamento\n","                                                    stratify=Y,                # estratificação para manter a distribuição dos rótulos igual entre treino e teste\n","                                                    shuffle=True)              # embaralhar os exemplos aleatoriamente"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qT0hVbqYyEbH"},"source":["1. **Seguir o tutorial pra ter acesso aos embedding de cada palavra**\n","\n","2. Converter as palavras para o próprio embedding durante o treinamento (1 frase por vez) -- criar funçao pra tal\n","  - usar  embedding.get_vector(palavra)\n","\n","3. pyTortch dataLoader - batchSize --> https://pytorch.org/docs/stable/data.html\n"]},{"cell_type":"code","metadata":{"id":"3qC2D1U3JKxv"},"source":["#words = [\"draw\", \"to\", \"the\", \"ground\"]\n","#teste = [label2Embedding(word) for word in words]\n","#teste"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qf8exmX6MXRs"},"source":["Creating model"]},{"cell_type":"code","metadata":{"id":"F9t2TFGwKZS2"},"source":["class Torch_Mean_Layer(nn.Module):\n","  '''Camada personalizada: calcula a média do tensor dentrada sobre a dimensão 1 (colunas).\n","     Retorna um vetor linha, onde cada elemento é a média dos elementos da coluna correspondente do tensor de entrada.\n","  '''\n","  def forward(self, x, dim=1):\n","    print(\"-----------------------\",x)\n","    x = torch.mean(x, dim=dim, keepdims=True)\n","    return x\n","\n","class mood_classifier(nn.Module):\n","  '''Modelo classificador de emoções\n","  '''\n","\n","  # ----------------------------------------------#\n","  # Método construtor\n","  def __init__(self, vocab_size, dim_embed, n_units): \n","    super().__init__()  \n","\n","    embedding_seq = [] # \n","    ann_seq       = [] # \n","    soft_seq      = []\n","\n","    #---------------------------------------------------------------#\n","    # Embedding step: sequência de operações para converter X --> h\n","    embedding_seq.append(Torch_Mean_Layer())\n","    #---------------------------------------------------------------#\n","\n","    #--------------------------------------------------------------------------#\n","    # ANN: Rede Neural Artifical Tradicional, com regressão logística na saída\n","    ann_seq.append(nn.Linear(dim_embed, n_units))\n","    ann_seq.append(nn.ReLU(inplace=True))\n","    ann_seq.append(nn.Linear(n_units, 6))\n","    \n","    #--------------------------------------------------------------------------#\n","    # Softmax :)\n","    soft_seq.append(nn.LogSoftmax(dim=1))\n","\n","    #--------------------------------------------------------------------------#\n","\n","    #--------------------------------------------------------------------------#\n","    # \"merge\" de todas as camamadas em uma layer sequencial \n","    # (uma sequência para cada etapa)\n","    self.embedding = nn.Sequential(*embedding_seq)     # etapa de embedding \n","    self.ann       = nn.Sequential(*ann_seq)           # etapa ANN\n","    self.soft      = nn.Sequential(*soft_seq)\n","    #--------------------------------------------------------------------------#\n","\n","\n","  def forward(self, x): \n","    '''Processamento realizado ao chamar y=modelo(x)\n","    '''\n","    x = self.embedding(x)  # aplica a etapa de embedding\n","    x = self.ann(x)        # passa o embedding médio pelas camadas da ANN\n","    x = x.view(-1,6)\n","    x = self.soft(x)\n","    return x  #Adcionar o softmax"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ss8SyoiBMaY8"},"source":["Training model"]},{"cell_type":"code","metadata":{"id":"w06ib3KfKgKU"},"source":["import tensorflow as tf\n","def train_loop(model, data, max_epochs = 1000, print_iters = 5):\n","  X_train, Y_train = data\n","  losses = []\n","  accs = []\n","  for i in range(max_epochs): # para cada época\n","\n","      #-----------------------------------#\n","      # INÍCIO DO WORKFLOW DO TREINAMENTO #\n","      # \n","      # Add mistura\n","\n","      Y_pred = model.forward(X_train)         # apresente os dados de entrada para o modelo, e obtenha a previsão    \n","      loss = criterion(Y_pred.view(-1, 6), Y_train.view(-1))       # calcule a perda (o custo, o erro)\n","      optimizer.zero_grad()                   # inicialize os gradientes\n","      loss.backward()                         # backpropagation sobre a perda atual (cálculo dos novos gradientes) \n","      optimizer.step()                        # atualização dos parâmetros da rede utilizando a regra do otimizador escolhido\n","      # FIM DO WORKFLOW DO TREINAMENTO    #\n","      #-----------------------------------#\n","\n","      # ------ Bloco Opcional ------ #\n","      # Salvando métricas\n","      losses.append(loss)                     # salvando a perda atual\n","      acc = calc_accuracy(Y_pred, Y_train)     # calcula a taxa de acerto atual\n","      accs.append(acc)\n","      \n","      # Imprimindo resultados parciais\n","      if i % print_iters ==0: # a cada 10 iterações\n","        print(f'epoch: {i:2}  loss: {loss.item():10.8f}') \n","      #-----------------------------------#\n","\n","  #----------------------------------------------------------------------------# \n","  print('\\n# Finished training!')\n","  print(f'# --> epoch: {i}  \\n# --> initial loss: {losses[0]:10.8f} ,  \\n# --> accuracy: {acc:2.8f} , \\n# --> final loss: {losses[-1]:10.8f}')\n","  \n","  # retornando resultados\n","  return model, losses, accs\n","\n","# Redefinindo cálculo da taxa de acerto \n","def calc_accuracy(y_pred, y_true):\n","  ''' Helper function para calcular a taxa de acerto deste exemplo.\n","  '''\n","  y_pred = torch.argmax(y_pred, dim=1)\n","  y_pred = y_pred.float()\n","  y_true = torch.squeeze(y_true) # tentar rexplicar dps\n","  y_pred = torch.squeeze(y_pred)\n","  num_hits  = torch.sum(y_pred==y_true).numpy()\n","  num_total =  float(y_true.numel())\n","  acc=  num_hits/num_total\n","  return acc"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cXJdUs0TNco9"},"source":["Now, we're convert everything to Tensors and instantiate our loss function"]},{"cell_type":"code","metadata":{"id":"1JfCBcGxQL2K"},"source":["x_train = np.vstack(X_train)\n","y_train = np.array(Y_train).reshape(-1,1)\n","x_train.shape, x_train[0].shape, y_train.shape, y_train[0].shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2wf3yRuXNizi"},"source":["import tensorflow as tf\n","\n","#data_train = (tf.constant(x_train, dtype=tf.float64), tf.constant(y_train, dtype=tf.float64))\n","data_train = (torch.float32(X_train), torch.float32(Y_train))\n","\n","Model = mood_classifier(vocab_size=len(word2idx), dim_embed=100, n_units=100)\n","print(Model)\n","\n","criterion = nn.NLLLoss() # cross entropy loss\n","optimizer = torch.optim.Adam(Model.parameters(), lr = 0.01) \n","\n","Model, losses, accs = train_loop(Model, data_train, max_epochs=330, print_iters=1) # note que o modelo é sobrescrito pela saída treinada"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OFfxQbKeYcRY"},"source":["--------------------------------------------------------------------------------------------------"]},{"cell_type":"code","metadata":{"id":"nzagyAB2VGKR"},"source":["X_train = torch.LongTensor(X_train)\n","Y_train = torch.LongTensor(Y_train)\n","X_test = np.vstack(X_test)\n","X_test = torch.LongTensor(X_test)\n","Y_test = np.array(Y_test).reshape(-1,1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IC425RVHVCw5"},"source":["Y_pred = torch.exp(Model.forward(X_test))         "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fm8CH5b8U_ua"},"source":["Y_pred = torch.argmax(Y_pred, dim=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QpRsLdh5n0yO"},"source":["dataset[\"Feeling\"].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fuFI3C5woXnt"},"source":["dataset[\"emotion_code\"].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CUPktCTpQQ9e"},"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix\n","\n","sns.set(font_scale=1.5)\n","\n","matriz_de_confusao = confusion_matrix(Y_pred, Y_test)\n","\n","#criando list com as emoções\n","emotion_class = ['Angry','Disgust','Fear','Happy','Sad','Surprise']\n","\n","df_matriz_de_confusao = pd.DataFrame(matriz_de_confusao, emotion_class, emotion_class)\n","\n","# confusion matrix daora TEM Q MELHORAR\n","def plot_conf_mat(y_test, y_preds, norm='true'):\n","   fig, ax = plt.subplots(figsize=(8, 6))\n","   ax = sns.heatmap(df_matriz_de_confusao,\n","                   annot=True,\n","                    fmt=\"d\",\n","                    cmap=\"YlOrRd\")\n","   plt.xlabel(\"Resultado previsto\")\n","   plt.ylabel(\"Resultado real\")\n","\n","\n","plot_conf_mat(Y_test, Y_pred)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BCUY0m-omsTs"},"source":["NICE GRAPHICS"]},{"cell_type":"code","metadata":{"id":"1c72jSwDmvKE"},"source":["def plot_loss_and_accuracy(losses, accs):\n","\n","  fig, ax_tuple = plt.subplots(1, 2, figsize=(16,6))\n","  fig.suptitle('Loss and accuracy')\n","\n","  for i, (y_label, y_values) in enumerate(zip(['CE loss','Accuracy'],[losses, accs])):\n","    ax_tuple[i].plot(range(len(y_values)),  y_values, label='train')\n","    ax_tuple[i].set_xlabel('epochs')\n","    ax_tuple[i].set_ylabel(y_label)\n","    ax_tuple[i].legend()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"frcaHCLHmw2R"},"source":["plot_loss_and_accuracy(losses, accs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Alze2r2iVpLg"},"source":["Instantiating another model"]},{"cell_type":"code","metadata":{"id":"eEAv5Vevm8Dw"},"source":["#model = torch.load(\"/content/drive/My Drive/Análise de Sentimentos/Projeto para 08-09/Models/model.pth\")\n","#model.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pWyLsC_8VtnG"},"source":["#data_train = (torch.LongTensor(x_train), torch.LongTensor(y_train))\n","\n","#model = model(model, vocab_size=len(word2idx), dim_embed=100, n_units=100)\n","\n","#criterion = nn.NLLLoss() # cross entropy loss\n","#optimizer = torch.optim.Adam(Model.parameters(), lr = 0.01) \n","\n","#model, losses, accs = train_loop(model, data_train, max_epochs=100, print_iters=1) # note que o modelo é sobrescrito pela saída treinada"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3bgIshr92fw7"},"source":["https://keras.io/examples/nlp/pretrained_word_embeddings/"]}]}