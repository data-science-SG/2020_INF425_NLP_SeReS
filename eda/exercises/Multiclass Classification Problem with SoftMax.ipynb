{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Multiclass Classification Problem with SoftMax and Bokeh.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"OAJa_FwAkiqp"},"source":["# Applying GloVE\n","\n","Firsts things firsts, this notebook will use the data distributed by Kaggle: https://www.kaggle.com/shainy/twitter-reviews-for-emotion-analysis\n","<br />\n","Resuming, it's about movie genre\n","<br /><br />\n","In this notebook, we will try to predict the movie genre using the **softmax function** and after, plotting using a bokeh graph"]},{"cell_type":"markdown","metadata":{"id":"2Os3Kj6MmGRU"},"source":["## Beginning of the work\n","\n","To begin, we will mount the connection with drive so we can acess the csv"]},{"cell_type":"code","metadata":{"id":"5a_PZcxTnFpO","executionInfo":{"status":"ok","timestamp":1603843451849,"user_tz":180,"elapsed":7311,"user":{"displayName":"Carlos Reinheimer","photoUrl":"","userId":"11551777167550877658"}},"outputId":"5ee8a8e2-ce7f-407c-cbbc-7d08d9373e8e","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# dataset\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline \n","# pd.set_option('display.max_colwidth', -1)\n","\n","# my functions\n","# import UtilsCarlos\n","# from UtilsCarlos import  criaDicio, criaVetor, fit_and_score, plot_conf_mat, convert\n","\n","# nltk\n","import nltk\n","from nltk.tokenize import TweetTokenizer\n","import re\n","from nltk import FreqDist\n","from nltk.tokenize import TweetTokenizer\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords \n","tknzr = TweetTokenizer()\n","stemmer = PorterStemmer()\n","nltk.download('punkt')\n","\n","# modelos\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn import tree\n","import tensorflow as tf\n","import torch\n","from torch import nn\n","\n","# evaluations\n","from sklearn.model_selection import train_test_split, cross_val_score\n","from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n","from sklearn.metrics import confusion_matrix, classification_report\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","from sklearn.metrics import plot_roc_curve\n","from sklearn.model_selection import cross_val_score\n","\n","# others\n","import json\n","import string\n","\n","# PyTorch\n","#import torch\n","#from torch import nn"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\carlo\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"code","metadata":{"id":"MQAebXvbOZia","executionInfo":{"status":"ok","timestamp":1603843456688,"user_tz":180,"elapsed":724,"user":{"displayName":"Carlos Reinheimer","photoUrl":"","userId":"11551777167550877658"}},"outputId":"8f7f9e8b-aabd-4214-a9d9-e142e19ee8df","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["nltk.download('stopwords')\n","stopwords = nltk.corpus.stopwords.words(\"english\")\n","token_espaço = nltk.tokenize.WhitespaceTokenizer()"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\carlo\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"]}]},{"cell_type":"code","metadata":{"id":"XHLCEe5iONyu","executionInfo":{"status":"ok","timestamp":1603843459040,"user_tz":180,"elapsed":882,"user":{"displayName":"Carlos Reinheimer","photoUrl":"","userId":"11551777167550877658"}}},"source":["# função de limpar texto\n","def cleanText(words, stem=False):\n","  \"\"\"\n","    Esta função recebe um text e retorna o mesmo, já tratado com stopwords & punctuation\n","  \"\"\"\n","  newWords = list()\n","  pontuacao = string,\n","  for word in words:\n","    word = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n","                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', word)\n","    words = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", word)\n","    if len(word) > 0 and words not in string.punctuation and word.lower() not in stopwords and word.lower != \"<br />\":\n","      if stem:\n","        word = stemmer.stem(word.lower())\n","        newWords.append(word)\n","      else:\n","        newWords.append(word.lower())\n","\n","  return newWords"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"QMEdrkjdOOmy","executionInfo":{"status":"ok","timestamp":1603843459042,"user_tz":180,"elapsed":879,"user":{"displayName":"Carlos Reinheimer","photoUrl":"","userId":"11551777167550877658"}}},"source":["# confusion matrix daora\n","def plot_conf_mat(y_test, y_preds, norm=\"false\"):\n","    fig, ax = plt.subplots(figsize=(3, 3))\n","    ax = sns.heatmap(confusion_matrix(y_test, y_preds, normalize=norm),\n","                    annot=True,\n","                    cbar=False)\n","    plt.xlabel(\"True label\")\n","    plt.ylabel(\"Predicted label\")"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"T7kT0ZfyYt0s","executionInfo":{"status":"ok","timestamp":1603843460103,"user_tz":180,"elapsed":619,"user":{"displayName":"Carlos Reinheimer","photoUrl":"","userId":"11551777167550877658"}}},"source":["def plot_loss_and_accuracy(losses, accs):\n","\n","  fig, ax_tuple = plt.subplots(1, 2, figsize=(16,6))\n","  fig.suptitle('Loss and accuracy')\n","\n","  for i, (y_label, y_values) in enumerate(zip(['BCE loss','Accuracy'],[losses, accs])):\n","    ax_tuple[i].plot(range(len(y_values)),  y_values, label='train')\n","    ax_tuple[i].set_xlabel('epochs')\n","    ax_tuple[i].set_ylabel(y_label)\n","    ax_tuple[i].legend()"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"iAKxRC_WV9Ox","executionInfo":{"status":"ok","timestamp":1603843460403,"user_tz":180,"elapsed":784,"user":{"displayName":"Carlos Reinheimer","photoUrl":"","userId":"11551777167550877658"}}},"source":["def label2Embedding(sentence):\n","  for word in sentence: \n","    if word in modelo.vocab:\n","      embed = modelo.get_vector(word)\n","      print(embed)\n","      return embed\n","    else:\n","      return 0\n","      #print(\"This word is not in the vocabuylary: \", word, \"\\n\")"],"execution_count":7,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","metadata":{"id":"E1FdpbXN_0rz","executionInfo":{"status":"error","timestamp":1601596765257,"user_tz":180,"elapsed":30069,"user":{"displayName":"Carlos Reinheimer","photoUrl":"","userId":"11551777167550877658"}},"outputId":"47b08b30-b06f-4a29-c5d1-56da75a99258","colab":{"base_uri":"https://localhost:8080/","height":231}},"source":["filename_txt = '../data/glove.6B.50d.txt'\n","\n","with open(filename_txt, 'r+', encoding=\"utf8\") as f:\n","  content = f.read()\n","  for i, l in enumerate(f):\n","    pass\n","    line = f'{i+1} 100'\n","    f.seek(0, 0)\n","    f.write(line.rstrip('\\r\\n') + '\\n' + content)\n","\n","with open(filename_txt) as f:\n","  for linha in range(10):\n","    print(next(f))"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["101 50\n\nthe 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 -0.6566 0.27843 -0.14767 -0.55677 0.14658 -0.0095095 0.011658 0.10204 -0.12792 -0.8443 -0.12181 -0.016801 -0.33279 -0.1552 -0.23131 -0.19181 -1.8823 -0.76746 0.099051 -0.42125 -0.19526 4.0071 -0.18594 -0.52287 -0.31681 0.00059213 0.0074449 0.17778 -0.15897 0.012041 -0.054223 -0.29871 -0.15749 -0.34758 -0.045637 -0.44251 0.18785 0.0027849 -0.18411 -0.11514 -0.78581\n\n, 0.013441 0.23682 -0.16899 0.40951 0.63812 0.47709 -0.42852 -0.55641 -0.364 -0.23938 0.13001 -0.063734 -0.39575 -0.48162 0.23291 0.090201 -0.13324 0.078639 -0.41634 -0.15428 0.10068 0.48891 0.31226 -0.1252 -0.037512 -1.5179 0.12612 -0.02442 -0.042961 -0.28351 3.5416 -0.11956 -0.014533 -0.1499 0.21864 -0.33412 -0.13872 0.31806 0.70358 0.44858 -0.080262 0.63003 0.32111 -0.46765 0.22786 0.36034 -0.37818 -0.56657 0.044691 0.30392\n\n. 0.15164 0.30177 -0.16763 0.17684 0.31719 0.33973 -0.43478 -0.31086 -0.44999 -0.29486 0.16608 0.11963 -0.41328 -0.42353 0.59868 0.28825 -0.11547 -0.041848 -0.67989 -0.25063 0.18472 0.086876 0.46582 0.015035 0.043474 -1.4671 -0.30384 -0.023441 0.30589 -0.21785 3.746 0.0042284 -0.18436 -0.46209 0.098329 -0.11907 0.23919 0.1161 0.41705 0.056763 -6.3681e-05 0.068987 0.087939 -0.10285 -0.13931 0.22314 -0.080803 -0.35652 0.016413 0.10216\n\nof 0.70853 0.57088 -0.4716 0.18048 0.54449 0.72603 0.18157 -0.52393 0.10381 -0.17566 0.078852 -0.36216 -0.11829 -0.83336 0.11917 -0.16605 0.061555 -0.012719 -0.56623 0.013616 0.22851 -0.14396 -0.067549 -0.38157 -0.23698 -1.7037 -0.86692 -0.26704 -0.2589 0.1767 3.8676 -0.1613 -0.13273 -0.68881 0.18444 0.0052464 -0.33874 -0.078956 0.24185 0.36576 -0.34727 0.28483 0.075693 -0.062178 -0.38988 0.22902 -0.21617 -0.22562 -0.093918 -0.80375\n\nto 0.68047 -0.039263 0.30186 -0.17792 0.42962 0.032246 -0.41376 0.13228 -0.29847 -0.085253 0.17118 0.22419 -0.10046 -0.43653 0.33418 0.67846 0.057204 -0.34448 -0.42785 -0.43275 0.55963 0.10032 0.18677 -0.26854 0.037334 -2.0932 0.22171 -0.39868 0.20912 -0.55725 3.8826 0.47466 -0.95658 -0.37788 0.20869 -0.32752 0.12751 0.088359 0.16351 -0.21634 -0.094375 0.018324 0.21048 -0.03088 -0.19722 0.082279 -0.09434 -0.073297 -0.064699 -0.26044\n\nand 0.26818 0.14346 -0.27877 0.016257 0.11384 0.69923 -0.51332 -0.47368 -0.33075 -0.13834 0.2702 0.30938 -0.45012 -0.4127 -0.09932 0.038085 0.029749 0.10076 -0.25058 -0.51818 0.34558 0.44922 0.48791 -0.080866 -0.10121 -1.3777 -0.10866 -0.23201 0.012839 -0.46508 3.8463 0.31362 0.13643 -0.52244 0.3302 0.33707 -0.35601 0.32431 0.12041 0.3512 -0.069043 0.36885 0.25168 -0.24517 0.25381 0.1367 -0.31178 -0.6321 -0.25028 -0.38097\n\nin 0.33042 0.24995 -0.60874 0.10923 0.036372 0.151 -0.55083 -0.074239 -0.092307 -0.32821 0.09598 -0.82269 -0.36717 -0.67009 0.42909 0.016496 -0.23573 0.12864 -1.0953 0.43334 0.57067 -0.1036 0.20422 0.078308 -0.42795 -1.7984 -0.27865 0.11954 -0.12689 0.031744 3.8631 -0.17786 -0.082434 -0.62698 0.26497 -0.057185 -0.073521 0.46103 0.30862 0.12498 -0.48609 -0.0080272 0.031184 -0.36576 -0.42699 0.42164 -0.11666 -0.50703 -0.027273 -0.53285\n\na 0.21705 0.46515 -0.46757 0.10082 1.0135 0.74845 -0.53104 -0.26256 0.16812 0.13182 -0.24909 -0.44185 -0.21739 0.51004 0.13448 -0.43141 -0.03123 0.20674 -0.78138 -0.20148 -0.097401 0.16088 -0.61836 -0.18504 -0.12461 -2.2526 -0.22321 0.5043 0.32257 0.15313 3.9636 -0.71365 -0.67012 0.28388 0.21738 0.14433 0.25926 0.23434 0.4274 -0.44451 0.13813 0.36973 -0.64289 0.024142 -0.039315 -0.26037 0.12017 -0.043782 0.41013 0.1796\n\n\" 0.25769 0.45629 -0.76974 -0.37679 0.59272 -0.063527 0.20545 -0.57385 -0.29009 -0.13662 0.32728 1.4719 -0.73681 -0.12036 0.71354 -0.46098 0.65248 0.48887 -0.51558 0.039951 -0.34307 -0.014087 0.86488 0.3546 0.7999 -1.4995 -1.8153 0.41128 0.23921 -0.43139 3.6623 -0.79834 -0.54538 0.16943 -0.82017 -0.3461 0.69495 -1.2256 -0.17992 -0.057474 0.030498 -0.39543 -0.38515 -1.0002 0.087599 -0.31009 -0.34677 -0.31438 0.75004 0.97065\n\n"]}]},{"cell_type":"code","metadata":{"id":"BNIRQsPYVlTg"},"source":["# gensin\n","from gensim.models import KeyedVectors\n","\n","filename_txt = '../data/glove.6B.50d.txt'\n","modelo = KeyedVectors.load_word2vec_format(filename_txt)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"L-LJvGSXOR25"},"source":["df = pd.read_csv(\"../data/data.csv\", encoding = \"ISO-8859-1\")\n","df.tail()"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       Sl no                                             Tweets  Search key  \\\n","10012  10016  Tweet #85: @Matteo tweeted \"@GameSpot @Frannkc...  irritating   \n","10013  10017  Tweet #86: @ðð§ð¢ð¬ð­ð¨ð§ tweet...  irritating   \n","10014  10018  Tweet #87: @Chowkidar Ricky Sharma tweeted \"@M...  irritating   \n","10015  10019  Tweet #88: @Katoe.EXE tweeted \"u know what i h...  irritating   \n","10016  10019  Tweet #88: @Katoe.EXE tweeted \"u know what i h...  irritating   \n","\n","      Feeling  \n","10012   angry  \n","10013   angry  \n","10014   angry  \n","10015   angry  \n","10016   angry  "],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sl no</th>\n      <th>Tweets</th>\n      <th>Search key</th>\n      <th>Feeling</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10012</th>\n      <td>10016</td>\n      <td>Tweet #85: @Matteo tweeted \"@GameSpot @Frannkc...</td>\n      <td>irritating</td>\n      <td>angry</td>\n    </tr>\n    <tr>\n      <th>10013</th>\n      <td>10017</td>\n      <td>Tweet #86: @ðð§ð¢ð¬ð­ð¨ð§ tweet...</td>\n      <td>irritating</td>\n      <td>angry</td>\n    </tr>\n    <tr>\n      <th>10014</th>\n      <td>10018</td>\n      <td>Tweet #87: @Chowkidar Ricky Sharma tweeted \"@M...</td>\n      <td>irritating</td>\n      <td>angry</td>\n    </tr>\n    <tr>\n      <th>10015</th>\n      <td>10019</td>\n      <td>Tweet #88: @Katoe.EXE tweeted \"u know what i h...</td>\n      <td>irritating</td>\n      <td>angry</td>\n    </tr>\n    <tr>\n      <th>10016</th>\n      <td>10019</td>\n      <td>Tweet #88: @Katoe.EXE tweeted \"u know what i h...</td>\n      <td>irritating</td>\n      <td>angry</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"xOoGjKv3W1tQ"},"source":["dataset = df.copy()"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"GNlORY0DXqKI"},"source":["dataset.drop(columns=[\"Sl no\", \"Search key\"])"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                  Tweets Feeling\n","0       #1: @fe ed \"RT @MirayaDizon1: Time is ticking...   happy\n","1       #2: @è®è± &ã¯ãã ed \"RT @ninjaryugo: ï¼...   happy\n","2       #3: @Ris â¡ ed \"Happy birthday to one smokin...   happy\n","3       #4: @ìì [ìì¯´ì¬ëë¡ë´] jwinnie is t...   happy\n","4       #5: @Madhurima wth u vcâ¥ ed \"Good morning d...   happy\n","...                                                  ...     ...\n","10012  Tweet #85: @Matteo tweeted \"@GameSpot @Frannkc...   angry\n","10013  Tweet #86: @ðð§ð¢ð¬ð­ð¨ð§ tweet...   angry\n","10014  Tweet #87: @Chowkidar Ricky Sharma tweeted \"@M...   angry\n","10015  Tweet #88: @Katoe.EXE tweeted \"u know what i h...   angry\n","10016  Tweet #88: @Katoe.EXE tweeted \"u know what i h...   angry\n","\n","[10017 rows x 2 columns]"],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweets</th>\n      <th>Feeling</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>#1: @fe ed \"RT @MirayaDizon1: Time is ticking...</td>\n      <td>happy</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>#2: @è®è± &amp;ã¯ãã ed \"RT @ninjaryugo: ï¼...</td>\n      <td>happy</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>#3: @Ris â¡ ed \"Happy birthday to one smokin...</td>\n      <td>happy</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>#4: @ìì [ìì¯´ì¬ëë¡ë´] jwinnie is t...</td>\n      <td>happy</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>#5: @Madhurima wth u vcâ¥ ed \"Good morning d...</td>\n      <td>happy</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>10012</th>\n      <td>Tweet #85: @Matteo tweeted \"@GameSpot @Frannkc...</td>\n      <td>angry</td>\n    </tr>\n    <tr>\n      <th>10013</th>\n      <td>Tweet #86: @ðð§ð¢ð¬ð­ð¨ð§ tweet...</td>\n      <td>angry</td>\n    </tr>\n    <tr>\n      <th>10014</th>\n      <td>Tweet #87: @Chowkidar Ricky Sharma tweeted \"@M...</td>\n      <td>angry</td>\n    </tr>\n    <tr>\n      <th>10015</th>\n      <td>Tweet #88: @Katoe.EXE tweeted \"u know what i h...</td>\n      <td>angry</td>\n    </tr>\n    <tr>\n      <th>10016</th>\n      <td>Tweet #88: @Katoe.EXE tweeted \"u know what i h...</td>\n      <td>angry</td>\n    </tr>\n  </tbody>\n</table>\n<p>10017 rows × 2 columns</p>\n</div>"},"metadata":{},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"qUq4P84qHtmz"},"source":["dataset['Feeling'] = pd.Categorical(dataset['Feeling'])"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"4N_clryjH1VU"},"source":["dataset['emotion_code'] = dataset['Feeling'].cat.codes\n","dataset.head()"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   Sl no                                             Tweets     Search key  \\\n","0      1   #1: @fe ed \"RT @MirayaDizon1: Time is ticking...  happy moments   \n","1      2   #2: @è®è± &ã¯ãã ed \"RT @ninjaryugo: ï¼...  happy moments   \n","2      3   #3: @Ris â¡ ed \"Happy birthday to one smokin...  happy moments   \n","3      4   #4: @ìì [ìì¯´ì¬ëë¡ë´] jwinnie is t...  happy moments   \n","4      5   #5: @Madhurima wth u vcâ¥ ed \"Good morning d...  happy moments   \n","\n","  Feeling  emotion_code  \n","0   happy             3  \n","1   happy             3  \n","2   happy             3  \n","3   happy             3  \n","4   happy             3  "],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sl no</th>\n      <th>Tweets</th>\n      <th>Search key</th>\n      <th>Feeling</th>\n      <th>emotion_code</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>#1: @fe ed \"RT @MirayaDizon1: Time is ticking...</td>\n      <td>happy moments</td>\n      <td>happy</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>#2: @è®è± &amp;ã¯ãã ed \"RT @ninjaryugo: ï¼...</td>\n      <td>happy moments</td>\n      <td>happy</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>#3: @Ris â¡ ed \"Happy birthday to one smokin...</td>\n      <td>happy moments</td>\n      <td>happy</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>#4: @ìì [ìì¯´ì¬ëë¡ë´] jwinnie is t...</td>\n      <td>happy moments</td>\n      <td>happy</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>#5: @Madhurima wth u vcâ¥ ed \"Good morning d...</td>\n      <td>happy moments</td>\n      <td>happy</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"9tnfKepfGgKr"},"source":["dataset[\"CleanText\"] = [tknzr.tokenize(word) for word in dataset[\"Tweets\"]]"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"xxe37sZVGso7"},"source":["dataset[\"CleanText\"] = [cleanText(word) for word in dataset[\"CleanText\"]]"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fgd1E4ubHZjj"},"source":["dataset.head()"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   Sl no                                             Tweets     Search key  \\\n","0      1   #1: @fe ed \"RT @MirayaDizon1: Time is ticking...  happy moments   \n","1      2   #2: @è®è± &ã¯ãã ed \"RT @ninjaryugo: ï¼...  happy moments   \n","2      3   #3: @Ris â¡ ed \"Happy birthday to one smokin...  happy moments   \n","3      4   #4: @ìì [ìì¯´ì¬ëë¡ë´] jwinnie is t...  happy moments   \n","4      5   #5: @Madhurima wth u vcâ¥ ed \"Good morning d...  happy moments   \n","\n","  Feeling  emotion_code                                          CleanText  \n","0   happy             3  [1, ed, rt, time, ticking, fast, relive, past,...  \n","1   happy             3  [2, @è, , ®, è, , ±, ã, , ¯, ã, , , ã, ,...  \n","2   happy             3  [3, â, , ¡, ed, happy, birthday, one, smokin,...  \n","3   happy             3  [4, @ì, , , ì, , , ì, , , ì, ¯, ´, ì, ,...  \n","4   happy             3  [5, wth, u, vcâ, , ¥, ed, good, morning, dear...  "],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sl no</th>\n      <th>Tweets</th>\n      <th>Search key</th>\n      <th>Feeling</th>\n      <th>emotion_code</th>\n      <th>CleanText</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>#1: @fe ed \"RT @MirayaDizon1: Time is ticking...</td>\n      <td>happy moments</td>\n      <td>happy</td>\n      <td>3</td>\n      <td>[1, ed, rt, time, ticking, fast, relive, past,...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>#2: @è®è± &amp;ã¯ãã ed \"RT @ninjaryugo: ï¼...</td>\n      <td>happy moments</td>\n      <td>happy</td>\n      <td>3</td>\n      <td>[2, @è, , ®, è, , ±, ã, , ¯, ã, , , ã, ,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>#3: @Ris â¡ ed \"Happy birthday to one smokin...</td>\n      <td>happy moments</td>\n      <td>happy</td>\n      <td>3</td>\n      <td>[3, â, , ¡, ed, happy, birthday, one, smokin,...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>#4: @ìì [ìì¯´ì¬ëë¡ë´] jwinnie is t...</td>\n      <td>happy moments</td>\n      <td>happy</td>\n      <td>3</td>\n      <td>[4, @ì, , , ì, , , ì, , , ì, ¯, ´, ì, ,...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>#5: @Madhurima wth u vcâ¥ ed \"Good morning d...</td>\n      <td>happy moments</td>\n      <td>happy</td>\n      <td>3</td>\n      <td>[5, wth, u, vcâ, , ¥, ed, good, morning, dear...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"Td2Eb7UtP60k"},"source":["#Criando EL VOCABULÁRIO (Com ajuda do código do sor em: Introdução ao PyTorch: da Regressão Linear à NLP com word-embeddings)\n","vocab_set = set() # será usado para gerar o vocabulário principal\n","max_len_doc = 0   # vamos medir o maior comprimento das mensagens envidas, em número de tokens\n","sum_len_doc = 0   # vamos medir o valor médio de palavras (tokens) por mensagem\n","min_word_len = 3  # comprimento mínimo de um token (em número de caracteres) para entrar no vocabulário \n","tokens_list = []  # salvar a lista de tokens\n","\n","for doc in dataset['CleanText']: # para cada documento do dataset\n","  #Pegando a palavra apenas se ela é maior que o comprimento mínimo\n","  for word in doc: \n","    if len(word)>=min_word_len:\n","      tokens_list.append(word)\n","\n","  # uso da função set: cria um conjunto dos elementos únicos da lista\n","  tokens_set = set(tokens_list)\n","  vocab_set = set.union(vocab_set, tokens_set) # adiciona elementos únicos que ainda não pertencem ao conjunto do vocabulário\n","\n","  sum_len_doc += len(word)\n","  if len(word) > max_len_doc:\n","    max_len_doc=len(word)\n","\n","print(f'Tamanho total do vocabulário: V={len(vocab_set)}')\n","print(f'Número de palavras do texto mais longo: {max_len_doc}')\n","print(f'Média de palavras por texto: {sum_len_doc/len(df):3.4f}')"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Tamanho total do vocabulário: V=26975\nNúmero de palavras do texto mais longo: 53\nMédia de palavras por texto: 3.6412\n"]}]},{"cell_type":"code","metadata":{"id":"kIhMThQX_7si","tags":["outputPrepend"]},"source":["word2idx = dict({})        # inicializa o dicionário\n","word2idx['<OOV>'] = 0      # índice da tag \"out of vocabulary\" é 0\n","word2idx['<PAD>'] = 1      # índice da tag \"padding token\" é 1\n","\n","for i, v in enumerate(sorted(vocab_set),start=2): # enumera o vocabulário em ordem alfabética, a partir do índice 2\n","  word2idx[v] = label2Embedding(v)\n","\n","# testando a conversão \"word to index\" com o dicionário:\n","print(f'index for \"<PAD>\": {word2idx[\"<PAD>\"]}')\n","print(f'index for \"action\": {word2idx[\"action\"]}')"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":[".8328e-01\n"," -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n"," -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n"," -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n","  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n","  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n","  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n"," -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n"," -2.6671e-01  9.2121e-01]\n","[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n"," -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n"," -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n"," -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n","  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n","  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n","  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n"," -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n"," -2.6671e-01  9.2121e-01]\n","[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n"," -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n"," -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n"," -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n","  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n","  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n","  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n"," -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n"," -2.6671e-01  9.2121e-01]\n","[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n"," -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n"," -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n"," -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n","  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n","  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n","  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n"," -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n"," -2.6671e-01  9.2121e-01]\n","[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n"," -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n"," -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n"," -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n","  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n","  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n","  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n"," -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n"," -2.6671e-01  9.2121e-01]\n","[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n"," -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n"," -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n"," -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n","  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n","  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n","  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n"," -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n"," -2.6671e-01  9.2121e-01]\n","[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n"," -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n"," -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n"," -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n","  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n","  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n","  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n"," -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n"," -2.6671e-01  9.2121e-01]\n","[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n"," -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n"," -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n"," -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n","  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n","  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n","  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n"," -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n"," -2.6671e-01  9.2121e-01]\n","[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n"," -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n"," -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n"," -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n","  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n","  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n","  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n"," -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n"," -2.6671e-01  9.2121e-01]\n","[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n"," -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n"," -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n"," -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n","  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n","  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n","  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n"," -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n"," -2.6671e-01  9.2121e-01]\n","[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n"," -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n"," -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n"," -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n","  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n","  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n","  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n"," -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n"," -2.6671e-01  9.2121e-01]\n","[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n"," -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n"," -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n"," -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n","  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n","  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n","  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n"," -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n"," -2.6671e-01  9.2121e-01]\n","[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n"," -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n"," -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n"," -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n","  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n","  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n","  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n"," -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n"," -2.6671e-01  9.2121e-01]\n","[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n"," -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n"," -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n"," -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n","  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n","  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n","  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n"," -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n"," -2.6671e-01  9.2121e-01]\n","[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n"," -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n"," -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n"," -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n","  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n","  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n","  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n"," -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n"," -2.6671e-01  9.2121e-01]\n","[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n"," -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n"," -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n"," -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n","  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n","  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n","  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n"," -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n"," -2.6671e-01  9.2121e-01]\n","[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n"," -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n"," -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n"," -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n","  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n","  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n","  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n"," -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n"," -2.6671e-01  9.2121e-01]\n","[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n"," -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n"," -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n"," -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n","  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n","  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n","  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n"," -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n"," -2.6671e-01  9.2121e-01]\n","[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n"," -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n"," -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n"," -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n","  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n","  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n","  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n"," -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n"," -2.6671e-01  9.2121e-01]\n","[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n"," -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n"," -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n"," -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n","  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n","  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n","  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n"," -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n"," -2.6671e-01  9.2121e-01]\n","[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n"," -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n"," -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n"," -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n","  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n","  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n","  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n"," -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n"," -2.6671e-01  9.2121e-01]\n","[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n"," -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n"," -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n"," -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n","  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n","  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n","  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n"," -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n"," -2.6671e-01  9.2121e-01]\n","[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n"," -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n"," -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n"," -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n","  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n","  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n","  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n"," -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n"," -2.6671e-01  9.2121e-01]\n","[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n"," -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n"," -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n"," -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n","  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n","  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n","  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n"," -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n"," -2.6671e-01  9.2121e-01]\n","[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n"," -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n"," -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n"," -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n","  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n","  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n","  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n"," -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n"," -2.6671e-01  9.2121e-01]\n","[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n"," -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n"," -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n"," -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n","  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n","  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n","  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n"," -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n"," -2.6671e-01  9.2121e-01]\n","[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n"," -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n"," -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n"," -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n","  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n","  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n","  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n"," -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n"," -2.6671e-01  9.2121e-01]\n","[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n"," -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n"," -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n"," -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n","  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n","  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n","  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n"," -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n"," -2.6671e-01  9.2121e-01]\n","[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n"," -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n"," -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n"," -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n","  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n","  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n","  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n"," -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n"," -2.6671e-01  9.2121e-01]\n","[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n"," -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n"," -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n"," -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n","  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n","  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n","  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n"," -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n"," -2.6671e-01  9.2121e-01]\n","[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n"," -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n"," -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n"," -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n","  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n","  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n","  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n"," -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n"," -2.6671e-01  9.2121e-01]\n","[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n"," -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n"," -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n"," -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n","  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n","  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n","  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n"," -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n"," -2.6671e-01  9.2121e-01]\n","index for \"<PAD>\": 1\n","index for \"action\": [ 0.21705   0.46515  -0.46757   0.10082   1.0135    0.74845  -0.53104\n"," -0.26256   0.16812   0.13182  -0.24909  -0.44185  -0.21739   0.51004\n","  0.13448  -0.43141  -0.03123   0.20674  -0.78138  -0.20148  -0.097401\n","  0.16088  -0.61836  -0.18504  -0.12461  -2.2526   -0.22321   0.5043\n","  0.32257   0.15313   3.9636   -0.71365  -0.67012   0.28388   0.21738\n","  0.14433   0.25926   0.23434   0.4274   -0.44451   0.13813   0.36973\n"," -0.64289   0.024142 -0.039315 -0.26037   0.12017  -0.043782  0.41013\n","  0.1796  ]\n"]}]},{"cell_type":"code","metadata":{"id":"3jIDIUf0JOQD"},"source":["idx2word = list(word2idx.keys()) # apenas transforma as chaves (palavras ordenadas) do dicionário word2idx em uma lista\n","\n","# testando a conversão \"index to word\":\n","print(f'word for index 0:    {idx2word[0]}')\n","print(f'word for index 100\": {idx2word[100]}')"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["word for index 0:    <OOV>\nword for index 100\": #75\n"]}]},{"cell_type":"code","metadata":{"id":"KiZFBVtlJx7k"},"source":["max_len = 25         # comprimento máximo da mensagem (em número de palavras)\n","encoded_docs = []    # inicializa a lista de documentos codificados\n","\n","for doc in dataset['CleanText']: # para cada texto\n","  encoded_d = [word2idx.get(t,word2idx['<OOV>']) for t in doc]    # codifica o documento usando o dicionário word2idx\n","  encoded_d += [word2idx['<PAD>']]*max(0, max_len-len(encoded_d))    # adiciona o padding, se necessário\n","  \n","  encoded_docs.append(encoded_d[:max_len])                           # trunca o documento e salva na lista de documentos codificados\n","\n","len(encoded_docs)  "],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10017"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"LRqo8v9KKQae"},"source":["dataset['CleanText'] = encoded_docs\n","dataset['CleanText'].tail()"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10012    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, [0.118...\n","10013    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n","10014    [0, 0, 0, 0, 0, 0, [0.21705, 0.46515, -0.46757...\n","10015    [0, 0, 0, 0, 0, 0, 0, [0.11891, 0.15255, -0.08...\n","10016    [0, 0, 0, 0, 0, 0, 0, [0.11891, 0.15255, -0.08...\n","Name: CleanText, dtype: object"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"UNjxLiLlK9nn"},"source":["X = np.vstack(dataset['CleanText'].apply(lambda x: np.array(x)))\n","Y = np.array(dataset['emotion_code']).reshape(-1,1)\n","X.shape, X[0].shape, Y.shape, Y[0].shape"],"execution_count":23,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-23-998411d5d345>:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n  X = np.vstack(dataset['CleanText'].apply(lambda x: np.array(x)))\n"]},{"output_type":"execute_result","data":{"text/plain":["((10017, 25), (25,), (10017, 1), (1,))"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"G21_Y4UxLER0"},"source":["train_size = 0.8    # percentual de exemplos para o treino\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(X,Y,                       # dataset para ser dividido, entrada X e saída Y\n","                                                    train_size=train_size,     # percentual resevado para o treinamento\n","                                                    stratify=Y,                # estratificação para manter a distribuição dos rótulos igual entre treino e teste\n","                                                    shuffle=True)              # embaralhar os exemplos aleatoriamente"],"execution_count":25,"outputs":[]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0, 0, 0, ..., 1, 1, 1],\n","       [0, 0, 0, ..., 1, 1, 1],\n","       [0, 0, 0, ..., 0, 0, 0],\n","       ...,\n","       [0, 0, 0, ..., 1, 1, 1],\n","       [0, 0,\n","        array([ 0.21705 ,  0.46515 , -0.46757 ,  0.10082 ,  1.0135  ,  0.74845 ,\n","       -0.53104 , -0.26256 ,  0.16812 ,  0.13182 , -0.24909 , -0.44185 ,\n","       -0.21739 ,  0.51004 ,  0.13448 , -0.43141 , -0.03123 ,  0.20674 ,\n","       -0.78138 , -0.20148 , -0.097401,  0.16088 , -0.61836 , -0.18504 ,\n","       -0.12461 , -2.2526  , -0.22321 ,  0.5043  ,  0.32257 ,  0.15313 ,\n","        3.9636  , -0.71365 , -0.67012 ,  0.28388 ,  0.21738 ,  0.14433 ,\n","        0.25926 ,  0.23434 ,  0.4274  , -0.44451 ,  0.13813 ,  0.36973 ,\n","       -0.64289 ,  0.024142, -0.039315, -0.26037 ,  0.12017 , -0.043782,\n","        0.41013 ,  0.1796  ], dtype=float32),\n","        ..., 0, 0, 0],\n","       [0, 0, 0, ..., 0, 0, 0]], dtype=object)"]},"metadata":{},"execution_count":26}],"source":["X_train"]},{"cell_type":"code","metadata":{"id":"F9t2TFGwKZS2"},"source":["\n","class Torch_Mean_Layer(nn.Module):\n","  '''Camada personalizada: calcula a média do tensor dentrada sobre a dimensão 1 (colunas).\n","     Retorna um vetor linha, onde cada elemento é a média dos elementos da coluna correspondente do tensor de entrada.\n","  '''\n","  def forward(self, x, dim=1):\n","    print(\"-----------------------\",x)\n","    x = torch.mean(x, dim=dim, keepdims=True)\n","    return x\n","\n","class mood_classifier(nn.Module):\n","  '''Modelo classificador de emoções\n","  '''\n","\n","  # ----------------------------------------------#\n","  # Método construtor\n","  def __init__(self, vocab_size, dim_embed, n_units): \n","    super().__init__()  \n","\n","    embedding_seq = [] # \n","    ann_seq       = [] # \n","    soft_seq      = []\n","\n","    #---------------------------------------------------------------#\n","    # Embedding step: sequência de operações para converter X --> h\n","    embedding_seq.append(Torch_Mean_Layer())\n","    #---------------------------------------------------------------#\n","\n","    #--------------------------------------------------------------------------#\n","    # ANN: Rede Neural Artifical Tradicional, com regressão logística na saída\n","    ann_seq.append(nn.Linear(dim_embed, n_units))\n","    ann_seq.append(nn.ReLU(inplace=True))\n","    ann_seq.append(nn.Linear(n_units, 6))\n","    \n","    #--------------------------------------------------------------------------#\n","    # Softmax :)\n","    soft_seq.append(nn.LogSoftmax(dim=1))\n","\n","    #--------------------------------------------------------------------------#\n","\n","    #--------------------------------------------------------------------------#\n","    # \"merge\" de todas as camamadas em uma layer sequencial \n","    # (uma sequência para cada etapa)\n","    self.embedding = nn.Sequential(*embedding_seq)     # etapa de embedding \n","    self.ann       = nn.Sequential(*ann_seq)           # etapa ANN\n","    self.soft      = nn.Sequential(*soft_seq)\n","    #--------------------------------------------------------------------------#\n","\n","\n","  def forward(self, x): \n","    '''Processamento realizado ao chamar y=modelo(x)\n","    '''\n","    x = self.embedding(x)  # aplica a etapa de embedding\n","    x = self.ann(x)        # passa o embedding médio pelas camadas da ANN\n","    x = x.view(-1,6)\n","    x = self.soft(x)\n","    return x  #Adcionar o softmax"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"w06ib3KfKgKU"},"source":["def train_loop(model, data, max_epochs = 1000, print_iters = 5):\n","  X_train, Y_train = data\n","  losses = []\n","  accs = []\n","  for i in range(max_epochs): # para cada época\n","\n","      #-----------------------------------#\n","      # INÍCIO DO WORKFLOW DO TREINAMENTO #\n","      # \n","      # Add mistura\n","\n","      Y_pred = model.forward(X_train)         # apresente os dados de entrada para o modelo, e obtenha a previsão    \n","      loss = criterion(Y_pred.view(-1, 6), Y_train.view(-1))       # calcule a perda (o custo, o erro)\n","      optimizer.zero_grad()                   # inicialize os gradientes\n","      loss.backward()                         # backpropagation sobre a perda atual (cálculo dos novos gradientes) \n","      optimizer.step()                        # atualização dos parâmetros da rede utilizando a regra do otimizador escolhido\n","      # FIM DO WORKFLOW DO TREINAMENTO    #\n","      #-----------------------------------#\n","\n","      # ------ Bloco Opcional ------ #\n","      # Salvando métricas\n","      losses.append(loss)                     # salvando a perda atual\n","      acc = calc_accuracy(Y_pred, Y_train)     # calcula a taxa de acerto atual\n","      accs.append(acc)\n","      \n","      # Imprimindo resultados parciais\n","      if i % print_iters ==0: # a cada 10 iterações\n","        print(f'epoch: {i:2}  loss: {loss.item():10.8f}') \n","      #-----------------------------------#\n","\n","  #----------------------------------------------------------------------------# \n","  print('\\n# Finished training!')\n","  print(f'# --> epoch: {i}  \\n# --> initial loss: {losses[0]:10.8f} ,  \\n# --> accuracy: {acc:2.8f} , \\n# --> final loss: {losses[-1]:10.8f}')\n","  \n","  # retornando resultados\n","  return model, losses, accs\n","\n","# Redefinindo cálculo da taxa de acerto \n","def calc_accuracy(y_pred, y_true):\n","  ''' Helper function para calcular a taxa de acerto deste exemplo.\n","  '''\n","  y_pred = torch.argmax(y_pred, dim=1)\n","  y_pred = y_pred.float()\n","  y_true = torch.squeeze(y_true) # tentar rexplicar dps\n","  y_pred = torch.squeeze(y_pred)\n","  num_hits  = torch.sum(y_pred==y_true).numpy()\n","  num_total =  float(y_true.numel())\n","  acc=  num_hits/num_total\n","  return acc"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"1JfCBcGxQL2K"},"source":["x_train = np.vstack(X_train)\n","y_train = np.array(Y_train).reshape(-1,1)\n","x_train.shape, x_train[0].shape, y_train.shape, y_train[0].shape"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((8013, 25), (25,), (8013, 1), (1,))"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"2wf3yRuXNizi"},"source":["#data_train = (tf.constant(X_train, dtype=tf.float64), tf.constant(Y_train, dtype=tf.float64))\n","#data_train = (torch.float32(X_train), torch.float32(Y_train))\n","X_train = torch.from_numpy(X_train)\n","Y_train = torch.from_numpy(Y_train)\n","\n","data_train = X_train.type(torch.float32), Y_train.type(torch.float32)\n","\n","Model = mood_classifier(vocab_size=len(word2idx), dim_embed=50, n_units=100)\n","print(Model)\n","\n","criterion = nn.NLLLoss() # cross entropy loss\n","optimizer = torch.optim.Adam(Model.parameters(), lr = 0.01) \n","\n","Model, losses, accs = train_loop(Model, data_train, max_epochs=330, print_iters=1) # note que o modelo é sobrescrito pela saída treinada"],"execution_count":31,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m<ipython-input-31-ec638dfb0da5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#data_train = (tf.constant(X_train, dtype=tf.float64), tf.constant(Y_train, dtype=tf.float64))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#data_train = (torch.float32(X_train), torch.float32(Y_train))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mY_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."]}]},{"cell_type":"code","metadata":{"id":"nzagyAB2VGKR"},"source":["X_train = torch.LongTensor(X_train)\n","Y_train = torch.LongTensor(Y_train)\n","X_test = np.vstack(X_test)\n","X_test = torch.LongTensor(X_test)\n","Y_test = np.array(Y_test).reshape(-1,1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IC425RVHVCw5"},"source":["Y_pred = torch.exp(Model.forward(X_test))         "],"execution_count":54,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'Model' is not defined","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m<ipython-input-54-a059dfae5e6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mY_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;31mNameError\u001b[0m: name 'Model' is not defined"]}]},{"cell_type":"code","metadata":{"id":"Fm8CH5b8U_ua"},"source":["Y_pred = torch.argmax(Y_pred, dim=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QpRsLdh5n0yO"},"source":["dataset[\"Feeling\"].value_counts()"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["happy       3928\n","sad         2849\n","angry       1341\n","fear         863\n","disgust      637\n","surprise     399\n","Name: Feeling, dtype: int64"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"fuFI3C5woXnt"},"source":["dataset[\"emotion_code\"].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CUPktCTpQQ9e"},"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix\n","\n","sns.set(font_scale=1.5)\n","\n","matriz_de_confusao = confusion_matrix(Y_pred, Y_test)\n","\n","#criando list com as emoções\n","emotion_class = ['Angry','Disgust','Fear','Happy','Sad','Surprise']\n","\n","df_matriz_de_confusao = pd.DataFrame(matriz_de_confusao, emotion_class, emotion_class)\n","\n","# confusion matrix daora TEM Q MELHORAR\n","def plot_conf_mat(y_test, y_preds, norm='true'):\n","   fig, ax = plt.subplots(figsize=(8, 6))\n","   ax = sns.heatmap(df_matriz_de_confusao,\n","                   annot=True,\n","                    fmt=\"d\",\n","                    cmap=\"YlOrRd\")\n","   plt.xlabel(\"Resultado previsto\")\n","   plt.ylabel(\"Resultado real\")\n","\n","\n","plot_conf_mat(Y_test, Y_pred)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BCUY0m-omsTs"},"source":["NICE GRAPHICS"]},{"cell_type":"code","metadata":{"id":"1c72jSwDmvKE"},"source":["def plot_loss_and_accuracy(losses, accs):\n","\n","  fig, ax_tuple = plt.subplots(1, 2, figsize=(16,6))\n","  fig.suptitle('Loss and accuracy')\n","\n","  for i, (y_label, y_values) in enumerate(zip(['CE loss','Accuracy'],[losses, accs])):\n","    ax_tuple[i].plot(range(len(y_values)),  y_values, label='train')\n","    ax_tuple[i].set_xlabel('epochs')\n","    ax_tuple[i].set_ylabel(y_label)\n","    ax_tuple[i].legend()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"frcaHCLHmw2R"},"source":["plot_loss_and_accuracy(losses, accs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Alze2r2iVpLg"},"source":["Instantiating another model"]},{"cell_type":"code","metadata":{"id":"eEAv5Vevm8Dw"},"source":["#model = torch.load(\"/content/drive/My Drive/Análise de Sentimentos/Projeto para 08-09/Models/model.pth\")\n","#model.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pWyLsC_8VtnG"},"source":["#data_train = (torch.LongTensor(x_train), torch.LongTensor(y_train))\n","\n","#model = model(model, vocab_size=len(word2idx), dim_embed=100, n_units=100)\n","\n","#criterion = nn.NLLLoss() # cross entropy loss\n","#optimizer = torch.optim.Adam(Model.parameters(), lr = 0.01) \n","\n","#model, losses, accs = train_loop(model, data_train, max_epochs=100, print_iters=1) # note que o modelo é sobrescrito pela saída treinada"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3bgIshr92fw7"},"source":["https://keras.io/examples/nlp/pretrained_word_embeddings/"]}]}