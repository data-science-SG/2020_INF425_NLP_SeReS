{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('Dev_Python': conda)",
   "metadata": {
    "interpreter": {
     "hash": "7fe04528e990a721b2ed72d37276d4ebca00a72e890c351a33e8e414defe75df"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\carlo\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\carlo\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Setup some variables\n",
    "\n",
    "tknzr = TweetTokenizer()\n",
    "stemmer = PorterStemmer()\n",
    "nltk.download('punkt')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "source": [
    "----------------------------------------------------------------------"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Declaring some functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# função de limpar texto\n",
    "def cleanText(words, stem=False):\n",
    "  \"\"\"\n",
    "    Esta função recebe um text e retorna o mesmo, já tratado com stopwords & punctuation\n",
    "  \"\"\"\n",
    "  newWords = list()\n",
    "  pontuacao = string,\n",
    "  for word in words:\n",
    "    word = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', word)\n",
    "    words = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", word)\n",
    "    if len(word) > 0 and words not in string.punctuation and word.lower() not in stopwords and word.lower != \"<br />\":\n",
    "      if stem:\n",
    "        word = stemmer.stem(word.lower())\n",
    "        newWords.append(word)\n",
    "      else:\n",
    "        newWords.append(word.lower())\n",
    "\n",
    "  return newWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix daora\n",
    "def plot_conf_mat(y_test, y_preds, norm=\"false\"):\n",
    "    fig, ax = plt.subplots(figsize=(3, 3))\n",
    "    ax = sns.heatmap(confusion_matrix(y_test, y_preds, normalize=norm),\n",
    "                    annot=True,\n",
    "                    cbar=False)\n",
    "    plt.xlabel(\"True label\")\n",
    "    plt.ylabel(\"Predicted label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_and_accuracy(losses, accs):\n",
    "\n",
    "  fig, ax_tuple = plt.subplots(1, 2, figsize=(16,6))\n",
    "  fig.suptitle('Loss and accuracy')\n",
    "\n",
    "  for i, (y_label, y_values) in enumerate(zip(['BCE loss','Accuracy'],[losses, accs])):\n",
    "    ax_tuple[i].plot(range(len(y_values)),  y_values, label='train')\n",
    "    ax_tuple[i].set_xlabel('epochs')\n",
    "    ax_tuple[i].set_ylabel(y_label)\n",
    "    ax_tuple[i].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label2Embedding(sentence):\n",
    "  for word in sentence: \n",
    "    if word in modelo.vocab:\n",
    "      embed = modelo.get_vector(word)\n",
    "      print(embed)\n",
    "      return embed\n",
    "    else:\n",
    "      return 0\n",
    "      #print(\"This word is not in the vocabuylary: \", word, \"\\n\")"
   ]
  },
  {
   "source": [
    "-------------------------------------------------------------------------------"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Importando o arquivo do GloVE de 50 dimensões e criando a variável *modelo* que será por onde iremos interagir com o *word embedding* já treinado"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_txt = '../data/glove.6B.50d.txt'\n",
    "modelo = KeyedVectors.load_word2vec_format(filename_txt)"
   ]
  },
  {
   "source": [
    "### Lendo o dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       Sl no                                             Tweets  Search key  \\\n",
       "10012  10016  Tweet #85: @Matteo tweeted \"@GameSpot @Frannkc...  irritating   \n",
       "10013  10017  Tweet #86: @ðð§ð¢ð¬ð­ð¨ð§ tweet...  irritating   \n",
       "10014  10018  Tweet #87: @Chowkidar Ricky Sharma tweeted \"@M...  irritating   \n",
       "10015  10019  Tweet #88: @Katoe.EXE tweeted \"u know what i h...  irritating   \n",
       "10016  10019  Tweet #88: @Katoe.EXE tweeted \"u know what i h...  irritating   \n",
       "\n",
       "      Feeling  \n",
       "10012   angry  \n",
       "10013   angry  \n",
       "10014   angry  \n",
       "10015   angry  \n",
       "10016   angry  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sl no</th>\n      <th>Tweets</th>\n      <th>Search key</th>\n      <th>Feeling</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10012</th>\n      <td>10016</td>\n      <td>Tweet #85: @Matteo tweeted \"@GameSpot @Frannkc...</td>\n      <td>irritating</td>\n      <td>angry</td>\n    </tr>\n    <tr>\n      <th>10013</th>\n      <td>10017</td>\n      <td>Tweet #86: @ðð§ð¢ð¬ð­ð¨ð§ tweet...</td>\n      <td>irritating</td>\n      <td>angry</td>\n    </tr>\n    <tr>\n      <th>10014</th>\n      <td>10018</td>\n      <td>Tweet #87: @Chowkidar Ricky Sharma tweeted \"@M...</td>\n      <td>irritating</td>\n      <td>angry</td>\n    </tr>\n    <tr>\n      <th>10015</th>\n      <td>10019</td>\n      <td>Tweet #88: @Katoe.EXE tweeted \"u know what i h...</td>\n      <td>irritating</td>\n      <td>angry</td>\n    </tr>\n    <tr>\n      <th>10016</th>\n      <td>10019</td>\n      <td>Tweet #88: @Katoe.EXE tweeted \"u know what i h...</td>\n      <td>irritating</td>\n      <td>angry</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 240
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/data.csv\", encoding = \"ISO-8859-1\")\n",
    "df.tail()"
   ]
  },
  {
   "source": [
    "### Copiando o dataset e fazendo transformações necessárias"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                              Tweets Feeling\n",
       "0   #1: @fe ed \"RT @MirayaDizon1: Time is ticking...   happy\n",
       "1   #2: @è®è± &ã¯ãã ed \"RT @ninjaryugo: ï¼...   happy\n",
       "2   #3: @Ris â¡ ed \"Happy birthday to one smokin...   happy\n",
       "3   #4: @ìì [ìì¯´ì¬ëë¡ë´] jwinnie is t...   happy\n",
       "4   #5: @Madhurima wth u vcâ¥ ed \"Good morning d...   happy"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweets</th>\n      <th>Feeling</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>#1: @fe ed \"RT @MirayaDizon1: Time is ticking...</td>\n      <td>happy</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>#2: @è®è± &amp;ã¯ãã ed \"RT @ninjaryugo: ï¼...</td>\n      <td>happy</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>#3: @Ris â¡ ed \"Happy birthday to one smokin...</td>\n      <td>happy</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>#4: @ìì [ìì¯´ì¬ëë¡ë´] jwinnie is t...</td>\n      <td>happy</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>#5: @Madhurima wth u vcâ¥ ed \"Good morning d...</td>\n      <td>happy</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 241
    }
   ],
   "source": [
    "dataset = df.copy()\n",
    "\n",
    "dataset = dataset.drop(columns=[\"Sl no\", \"Search key\"])\n",
    "dataset.head()"
   ]
  },
  {
   "source": [
    "### Criando uma categoria com o *pd.Categorical* para cada emoção na tabela de emoções, dessa forma, teremos algo do tipo: happy - 1 | angry - 2 | sad = 3\n",
    "\n",
    "Aplicando essas categorias na coluna *emotion_code*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                              Tweets Feeling  emotion_code\n",
       "0   #1: @fe ed \"RT @MirayaDizon1: Time is ticking...   happy             3\n",
       "1   #2: @è®è± &ã¯ãã ed \"RT @ninjaryugo: ï¼...   happy             3\n",
       "2   #3: @Ris â¡ ed \"Happy birthday to one smokin...   happy             3\n",
       "3   #4: @ìì [ìì¯´ì¬ëë¡ë´] jwinnie is t...   happy             3\n",
       "4   #5: @Madhurima wth u vcâ¥ ed \"Good morning d...   happy             3"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweets</th>\n      <th>Feeling</th>\n      <th>emotion_code</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>#1: @fe ed \"RT @MirayaDizon1: Time is ticking...</td>\n      <td>happy</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>#2: @è®è± &amp;ã¯ãã ed \"RT @ninjaryugo: ï¼...</td>\n      <td>happy</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>#3: @Ris â¡ ed \"Happy birthday to one smokin...</td>\n      <td>happy</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>#4: @ìì [ìì¯´ì¬ëë¡ë´] jwinnie is t...</td>\n      <td>happy</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>#5: @Madhurima wth u vcâ¥ ed \"Good morning d...</td>\n      <td>happy</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 242
    }
   ],
   "source": [
    "dataset['Feeling'] = pd.Categorical(dataset['Feeling'])\n",
    "dataset['emotion_code'] = dataset['Feeling'].cat.codes\n",
    "dataset.head()"
   ]
  },
  {
   "source": [
    "### Tokenizando os tweets com o *tknzr.tokenize*, e logo após, limpando os tokens com a função *cleanText*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                              Tweets Feeling  emotion_code  \\\n",
       "0   #1: @fe ed \"RT @MirayaDizon1: Time is ticking...   happy             3   \n",
       "1   #2: @è®è± &ã¯ãã ed \"RT @ninjaryugo: ï¼...   happy             3   \n",
       "2   #3: @Ris â¡ ed \"Happy birthday to one smokin...   happy             3   \n",
       "3   #4: @ìì [ìì¯´ì¬ëë¡ë´] jwinnie is t...   happy             3   \n",
       "4   #5: @Madhurima wth u vcâ¥ ed \"Good morning d...   happy             3   \n",
       "\n",
       "                                           CleanText  \n",
       "0  [1, ed, rt, time, ticking, fast, relive, past,...  \n",
       "1  [2, @è, , ®, è, , ±, ã, , ¯, ã, , , ã, ,...  \n",
       "2  [3, â, , ¡, ed, happy, birthday, one, smokin,...  \n",
       "3  [4, @ì, , , ì, , , ì, , , ì, ¯, ´, ì, ,...  \n",
       "4  [5, wth, u, vcâ, , ¥, ed, good, morning, dear...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweets</th>\n      <th>Feeling</th>\n      <th>emotion_code</th>\n      <th>CleanText</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>#1: @fe ed \"RT @MirayaDizon1: Time is ticking...</td>\n      <td>happy</td>\n      <td>3</td>\n      <td>[1, ed, rt, time, ticking, fast, relive, past,...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>#2: @è®è± &amp;ã¯ãã ed \"RT @ninjaryugo: ï¼...</td>\n      <td>happy</td>\n      <td>3</td>\n      <td>[2, @è, , ®, è, , ±, ã, , ¯, ã, , , ã, ,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>#3: @Ris â¡ ed \"Happy birthday to one smokin...</td>\n      <td>happy</td>\n      <td>3</td>\n      <td>[3, â, , ¡, ed, happy, birthday, one, smokin,...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>#4: @ìì [ìì¯´ì¬ëë¡ë´] jwinnie is t...</td>\n      <td>happy</td>\n      <td>3</td>\n      <td>[4, @ì, , , ì, , , ì, , , ì, ¯, ´, ì, ,...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>#5: @Madhurima wth u vcâ¥ ed \"Good morning d...</td>\n      <td>happy</td>\n      <td>3</td>\n      <td>[5, wth, u, vcâ, , ¥, ed, good, morning, dear...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 243
    }
   ],
   "source": [
    "dataset[\"CleanText\"] = [tknzr.tokenize(word) for word in dataset[\"Tweets\"]]\n",
    "\n",
    "dataset[\"CleanText\"] = [cleanText(word) for word in dataset[\"CleanText\"]]\n",
    "dataset.head()"
   ]
  },
  {
   "source": [
    "### Criando o vocabulário..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tamanho total do vocabulário: V=26975\nNúmero de palavras do texto mais longo: 53\nMédia de palavras por texto: 3.6412\n"
     ]
    }
   ],
   "source": [
    "#Criando EL VOCABULÁRIO (Com ajuda do código do sor em: Introdução ao PyTorch: da Regressão Linear à NLP com word-embeddings)\n",
    "vocab_set = set() # será usado para gerar o vocabulário principal\n",
    "max_len_doc = 0   # vamos medir o maior comprimento das mensagens envidas, em número de tokens\n",
    "sum_len_doc = 0   # vamos medir o valor médio de palavras (tokens) por mensagem\n",
    "min_word_len = 3  # comprimento mínimo de um token (em número de caracteres) para entrar no vocabulário \n",
    "tokens_list = []  # salvar a lista de tokens\n",
    "\n",
    "for doc in dataset['CleanText']: # para cada documento do dataset\n",
    "  #Pegando a palavra apenas se ela é maior que o comprimento mínimo\n",
    "  for word in doc: \n",
    "    if len(word)>=min_word_len:\n",
    "      tokens_list.append(word)\n",
    "\n",
    "  # uso da função set: cria um conjunto dos elementos únicos da lista\n",
    "  tokens_set = set(tokens_list) # i.e x = set(('text1', 'text2', 'text3'))  |  x => {'text1', 'text2', 'text3'}aaaaaaaaaaaaaaaaa\n",
    "  vocab_set = set.union(vocab_set, tokens_set) # adiciona elementos únicos que ainda não pertencem ao conjunto do vocabulário\n",
    "\n",
    "  sum_len_doc += len(word)\n",
    "  if len(word) > max_len_doc:\n",
    "    max_len_doc=len(word)\n",
    "\n",
    "print(f'Tamanho total do vocabulário: V={len(vocab_set)}')\n",
    "print(f'Número de palavras do texto mais longo: {max_len_doc}')\n",
    "print(f'Média de palavras por texto: {sum_len_doc/len(df):3.4f}')"
   ]
  },
  {
   "source": [
    "### Something"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "index for \"<PAD>\": 1\nindex for \"action\": 3517\n"
     ]
    }
   ],
   "source": [
    "word2idx = dict({})        # inicializa o dicionário\n",
    "word2idx['<OOV>'] = 0      # índice da tag \"out of vocabulary\" é 0\n",
    "word2idx['<PAD>'] = 1      # índice da tag \"padding token\" é 1\n",
    "\n",
    "for i, v in enumerate(sorted(vocab_set),start=2): # enumera o vocabulário em ordem alfabética, a partir do índice 2 | i.e x = ('apple', 'banana', 'cherry') | x => [(0, 'apple'), (1, 'banana'), (2, 'cherry')]\n",
    "  word2idx[v] = i\n",
    "\n",
    "# testando a conversão \"word to index\" com o dicionário:\n",
    "print(f'index for \"<PAD>\": {word2idx[\"<PAD>\"]}')\n",
    "print(f'index for \"action\": {word2idx[\"action\"]}')"
   ]
  },
  {
   "source": [
    "### Transforma as palavras ordenadas do dicionário de *index* para uma lista"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "word for index 0:    <OOV>\nword for index 100\": #flamingos\n"
     ]
    }
   ],
   "source": [
    "idx2word = list(word2idx.keys()) # apenas transforma as chaves (palavras ordenadas) do dicionário word2idx em uma lista\n",
    "\n",
    "# testando a conversão \"index to word\":\n",
    "print(f'word for index 0:    {idx2word[0]}')\n",
    "print(f'word for index 100\": {idx2word[666]}')"
   ]
  },
  {
   "source": [
    "### Conversão dos textos em uma sequência de índices (correspondente ao token do texto)\n",
    "\n",
    "Limitar o tamanho máximo de um texto com ***max_len*** (truncar mensagem) e completar com <PAD> todos os textos que não cumprirem esse tamanho, para então termos variáveis do mesmo tamanho"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " -2.7443e-03 -1.8298e-02\n",
      " -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n",
      " -2.6671e-01  9.2121e-01]\n",
      "[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n",
      " -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n",
      " -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n",
      " -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n",
      "  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n",
      "  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n",
      "  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n",
      " -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n",
      " -2.6671e-01  9.2121e-01]\n",
      "[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n",
      " -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n",
      " -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n",
      " -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n",
      "  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n",
      "  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n",
      "  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n",
      " -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n",
      " -2.6671e-01  9.2121e-01]\n",
      "[ 1.5164e-01  3.0177e-01 -1.6763e-01  1.7684e-01  3.1719e-01  3.3973e-01\n",
      " -4.3478e-01 -3.1086e-01 -4.4999e-01 -2.9486e-01  1.6608e-01  1.1963e-01\n",
      " -4.1328e-01 -4.2353e-01  5.9868e-01  2.8825e-01 -1.1547e-01 -4.1848e-02\n",
      " -6.7989e-01 -2.5063e-01  1.8472e-01  8.6876e-02  4.6582e-01  1.5035e-02\n",
      "  4.3474e-02 -1.4671e+00 -3.0384e-01 -2.3441e-02  3.0589e-01 -2.1785e-01\n",
      "  3.7460e+00  4.2284e-03 -1.8436e-01 -4.6209e-01  9.8329e-02 -1.1907e-01\n",
      "  2.3919e-01  1.1610e-01  4.1705e-01  5.6763e-02 -6.3681e-05  6.8987e-02\n",
      "  8.7939e-02 -1.0285e-01 -1.3931e-01  2.2314e-01 -8.0803e-02 -3.5652e-01\n",
      "  1.6413e-02  1.0216e-01]\n",
      "[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n",
      " -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n",
      " -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n",
      " -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n",
      "  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n",
      "  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n",
      "  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n",
      " -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n",
      " -2.6671e-01  9.2121e-01]\n",
      "[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n",
      " -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n",
      " -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n",
      " -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n",
      "  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n",
      "  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n",
      "  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n",
      " -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n",
      " -2.6671e-01  9.2121e-01]\n",
      "[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n",
      " -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n",
      " -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n",
      " -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n",
      "  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n",
      "  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n",
      "  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n",
      " -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n",
      " -2.6671e-01  9.2121e-01]\n",
      "[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n",
      " -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n",
      " -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n",
      " -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n",
      "  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n",
      "  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n",
      "  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n",
      " -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n",
      " -2.6671e-01  9.2121e-01]\n",
      "[ 0.21705   0.46515  -0.46757   0.10082   1.0135    0.74845  -0.53104\n",
      " -0.26256   0.16812   0.13182  -0.24909  -0.44185  -0.21739   0.51004\n",
      "  0.13448  -0.43141  -0.03123   0.20674  -0.78138  -0.20148  -0.097401\n",
      "  0.16088  -0.61836  -0.18504  -0.12461  -2.2526   -0.22321   0.5043\n",
      "  0.32257   0.15313   3.9636   -0.71365  -0.67012   0.28388   0.21738\n",
      "  0.14433   0.25926   0.23434   0.4274   -0.44451   0.13813   0.36973\n",
      " -0.64289   0.024142 -0.039315 -0.26037   0.12017  -0.043782  0.41013\n",
      "  0.1796  ]\n",
      "[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n",
      " -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n",
      " -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n",
      " -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n",
      "  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n",
      "  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n",
      "  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n",
      " -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n",
      " -2.6671e-01  9.2121e-01]\n",
      "[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n",
      " -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n",
      " -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n",
      " -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n",
      "  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n",
      "  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n",
      "  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n",
      " -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n",
      " -2.6671e-01  9.2121e-01]\n",
      "[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n",
      " -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n",
      " -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n",
      " -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n",
      "  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n",
      "  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n",
      "  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n",
      " -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n",
      " -2.6671e-01  9.2121e-01]\n",
      "[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n",
      " -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n",
      " -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n",
      " -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n",
      "  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n",
      "  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n",
      "  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n",
      " -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n",
      " -2.6671e-01  9.2121e-01]\n",
      "[ 0.21705   0.46515  -0.46757   0.10082   1.0135    0.74845  -0.53104\n",
      " -0.26256   0.16812   0.13182  -0.24909  -0.44185  -0.21739   0.51004\n",
      "  0.13448  -0.43141  -0.03123   0.20674  -0.78138  -0.20148  -0.097401\n",
      "  0.16088  -0.61836  -0.18504  -0.12461  -2.2526   -0.22321   0.5043\n",
      "  0.32257   0.15313   3.9636   -0.71365  -0.67012   0.28388   0.21738\n",
      "  0.14433   0.25926   0.23434   0.4274   -0.44451   0.13813   0.36973\n",
      " -0.64289   0.024142 -0.039315 -0.26037   0.12017  -0.043782  0.41013\n",
      "  0.1796  ]\n",
      "[ 0.21705   0.46515  -0.46757   0.10082   1.0135    0.74845  -0.53104\n",
      " -0.26256   0.16812   0.13182  -0.24909  -0.44185  -0.21739   0.51004\n",
      "  0.13448  -0.43141  -0.03123   0.20674  -0.78138  -0.20148  -0.097401\n",
      "  0.16088  -0.61836  -0.18504  -0.12461  -2.2526   -0.22321   0.5043\n",
      "  0.32257   0.15313   3.9636   -0.71365  -0.67012   0.28388   0.21738\n",
      "  0.14433   0.25926   0.23434   0.4274   -0.44451   0.13813   0.36973\n",
      " -0.64289   0.024142 -0.039315 -0.26037   0.12017  -0.043782  0.41013\n",
      "  0.1796  ]\n",
      "[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n",
      " -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n",
      " -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n",
      " -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n",
      "  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n",
      "  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n",
      "  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n",
      " -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n",
      " -2.6671e-01  9.2121e-01]\n",
      "[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n",
      " -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n",
      " -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n",
      " -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n",
      "  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n",
      "  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n",
      "  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n",
      " -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n",
      " -2.6671e-01  9.2121e-01]\n",
      "[ 1.5164e-01  3.0177e-01 -1.6763e-01  1.7684e-01  3.1719e-01  3.3973e-01\n",
      " -4.3478e-01 -3.1086e-01 -4.4999e-01 -2.9486e-01  1.6608e-01  1.1963e-01\n",
      " -4.1328e-01 -4.2353e-01  5.9868e-01  2.8825e-01 -1.1547e-01 -4.1848e-02\n",
      " -6.7989e-01 -2.5063e-01  1.8472e-01  8.6876e-02  4.6582e-01  1.5035e-02\n",
      "  4.3474e-02 -1.4671e+00 -3.0384e-01 -2.3441e-02  3.0589e-01 -2.1785e-01\n",
      "  3.7460e+00  4.2284e-03 -1.8436e-01 -4.6209e-01  9.8329e-02 -1.1907e-01\n",
      "  2.3919e-01  1.1610e-01  4.1705e-01  5.6763e-02 -6.3681e-05  6.8987e-02\n",
      "  8.7939e-02 -1.0285e-01 -1.3931e-01  2.2314e-01 -8.0803e-02 -3.5652e-01\n",
      "  1.6413e-02  1.0216e-01]\n",
      "[ 1.5164e-01  3.0177e-01 -1.6763e-01  1.7684e-01  3.1719e-01  3.3973e-01\n",
      " -4.3478e-01 -3.1086e-01 -4.4999e-01 -2.9486e-01  1.6608e-01  1.1963e-01\n",
      " -4.1328e-01 -4.2353e-01  5.9868e-01  2.8825e-01 -1.1547e-01 -4.1848e-02\n",
      " -6.7989e-01 -2.5063e-01  1.8472e-01  8.6876e-02  4.6582e-01  1.5035e-02\n",
      "  4.3474e-02 -1.4671e+00 -3.0384e-01 -2.3441e-02  3.0589e-01 -2.1785e-01\n",
      "  3.7460e+00  4.2284e-03 -1.8436e-01 -4.6209e-01  9.8329e-02 -1.1907e-01\n",
      "  2.3919e-01  1.1610e-01  4.1705e-01  5.6763e-02 -6.3681e-05  6.8987e-02\n",
      "  8.7939e-02 -1.0285e-01 -1.3931e-01  2.2314e-01 -8.0803e-02 -3.5652e-01\n",
      "  1.6413e-02  1.0216e-01]\n",
      "[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n",
      " -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n",
      " -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n",
      " -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n",
      "  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n",
      "  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n",
      "  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n",
      " -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n",
      " -2.6671e-01  9.2121e-01]\n",
      "[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n",
      " -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n",
      " -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n",
      " -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n",
      "  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n",
      "  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n",
      "  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n",
      " -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n",
      " -2.6671e-01  9.2121e-01]\n",
      "[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n",
      " -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n",
      " -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n",
      " -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n",
      "  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n",
      "  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n",
      "  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n",
      " -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n",
      " -2.6671e-01  9.2121e-01]\n",
      "[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n",
      " -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n",
      " -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n",
      " -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n",
      "  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n",
      "  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n",
      "  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n",
      " -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n",
      " -2.6671e-01  9.2121e-01]\n",
      "[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n",
      " -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n",
      " -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n",
      " -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n",
      "  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n",
      "  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n",
      "  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n",
      " -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n",
      " -2.6671e-01  9.2121e-01]\n",
      "[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n",
      " -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n",
      " -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n",
      " -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n",
      "  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n",
      "  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n",
      "  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n",
      " -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n",
      " -2.6671e-01  9.2121e-01]\n",
      "[ 0.21705   0.46515  -0.46757   0.10082   1.0135    0.74845  -0.53104\n",
      " -0.26256   0.16812   0.13182  -0.24909  -0.44185  -0.21739   0.51004\n",
      "  0.13448  -0.43141  -0.03123   0.20674  -0.78138  -0.20148  -0.097401\n",
      "  0.16088  -0.61836  -0.18504  -0.12461  -2.2526   -0.22321   0.5043\n",
      "  0.32257   0.15313   3.9636   -0.71365  -0.67012   0.28388   0.21738\n",
      "  0.14433   0.25926   0.23434   0.4274   -0.44451   0.13813   0.36973\n",
      " -0.64289   0.024142 -0.039315 -0.26037   0.12017  -0.043782  0.41013\n",
      "  0.1796  ]\n",
      "[ 0.21705   0.46515  -0.46757   0.10082   1.0135    0.74845  -0.53104\n",
      " -0.26256   0.16812   0.13182  -0.24909  -0.44185  -0.21739   0.51004\n",
      "  0.13448  -0.43141  -0.03123   0.20674  -0.78138  -0.20148  -0.097401\n",
      "  0.16088  -0.61836  -0.18504  -0.12461  -2.2526   -0.22321   0.5043\n",
      "  0.32257   0.15313   3.9636   -0.71365  -0.67012   0.28388   0.21738\n",
      "  0.14433   0.25926   0.23434   0.4274   -0.44451   0.13813   0.36973\n",
      " -0.64289   0.024142 -0.039315 -0.26037   0.12017  -0.043782  0.41013\n",
      "  0.1796  ]\n",
      "[ 0.21705   0.46515  -0.46757   0.10082   1.0135    0.74845  -0.53104\n",
      " -0.26256   0.16812   0.13182  -0.24909  -0.44185  -0.21739   0.51004\n",
      "  0.13448  -0.43141  -0.03123   0.20674  -0.78138  -0.20148  -0.097401\n",
      "  0.16088  -0.61836  -0.18504  -0.12461  -2.2526   -0.22321   0.5043\n",
      "  0.32257   0.15313   3.9636   -0.71365  -0.67012   0.28388   0.21738\n",
      "  0.14433   0.25926   0.23434   0.4274   -0.44451   0.13813   0.36973\n",
      " -0.64289   0.024142 -0.039315 -0.26037   0.12017  -0.043782  0.41013\n",
      "  0.1796  ]\n",
      "[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n",
      " -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n",
      " -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n",
      " -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n",
      "  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n",
      "  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n",
      "  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n",
      " -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n",
      " -2.6671e-01  9.2121e-01]\n",
      "[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n",
      " -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n",
      " -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n",
      " -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n",
      "  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n",
      "  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n",
      "  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n",
      " -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n",
      " -2.6671e-01  9.2121e-01]\n",
      "[ 0.21705   0.46515  -0.46757   0.10082   1.0135    0.74845  -0.53104\n",
      " -0.26256   0.16812   0.13182  -0.24909  -0.44185  -0.21739   0.51004\n",
      "  0.13448  -0.43141  -0.03123   0.20674  -0.78138  -0.20148  -0.097401\n",
      "  0.16088  -0.61836  -0.18504  -0.12461  -2.2526   -0.22321   0.5043\n",
      "  0.32257   0.15313   3.9636   -0.71365  -0.67012   0.28388   0.21738\n",
      "  0.14433   0.25926   0.23434   0.4274   -0.44451   0.13813   0.36973\n",
      " -0.64289   0.024142 -0.039315 -0.26037   0.12017  -0.043782  0.41013\n",
      "  0.1796  ]\n",
      "[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n",
      " -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n",
      " -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n",
      " -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n",
      "  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n",
      "  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n",
      "  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n",
      " -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n",
      " -2.6671e-01  9.2121e-01]\n",
      "[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n",
      " -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n",
      " -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n",
      " -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n",
      "  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n",
      "  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n",
      "  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n",
      " -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n",
      " -2.6671e-01  9.2121e-01]\n",
      "[ 0.21705   0.46515  -0.46757   0.10082   1.0135    0.74845  -0.53104\n",
      " -0.26256   0.16812   0.13182  -0.24909  -0.44185  -0.21739   0.51004\n",
      "  0.13448  -0.43141  -0.03123   0.20674  -0.78138  -0.20148  -0.097401\n",
      "  0.16088  -0.61836  -0.18504  -0.12461  -2.2526   -0.22321   0.5043\n",
      "  0.32257   0.15313   3.9636   -0.71365  -0.67012   0.28388   0.21738\n",
      "  0.14433   0.25926   0.23434   0.4274   -0.44451   0.13813   0.36973\n",
      " -0.64289   0.024142 -0.039315 -0.26037   0.12017  -0.043782  0.41013\n",
      "  0.1796  ]\n",
      "[ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n",
      " -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n",
      " -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n",
      " -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n",
      "  1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n",
      "  3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n",
      "  5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n",
      " -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n",
      " -2.6671e-01  9.2121e-01]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10017"
      ]
     },
     "metadata": {},
     "execution_count": 247
    }
   ],
   "source": [
    "max_len = 53         # comprimento máximo da mensagem (em número de palavras)\n",
    "encoded_docs = []    # inicializa a lista de documentos codificados\n",
    "\n",
    "for token in dataset['CleanText']: # para cada token\n",
    "  encoded_d = [label2Embedding(t) for t in token]\n",
    "\n",
    "  # adiciona o padding, se necessário\n",
    "  encoded_d += [word2idx['<PAD>']]*max(0, max_len-len(encoded_d)) \n",
    "  # trunca o documento e salva na lista de documentos codificados\n",
    "  encoded_docs.append(encoded_d[:max_len]) \n",
    "\n",
    "len(encoded_docs)  "
   ]
  },
  {
   "source": [
    "### Agora os textos são os documentos codificados"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10012    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, [0.118...\n",
       "10013    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "10014    [0, 0, 0, 0, 0, 0, [0.21705, 0.46515, -0.46757...\n",
       "10015    [0, 0, 0, 0, 0, 0, 0, [0.11891, 0.15255, -0.08...\n",
       "10016    [0, 0, 0, 0, 0, 0, 0, [0.11891, 0.15255, -0.08...\n",
       "Name: X, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 248
    }
   ],
   "source": [
    "dataset['X'] = encoded_docs\n",
    "dataset['X'].tail()"
   ]
  },
  {
   "source": [
    "### Verificando um exemplo de uma frase codificada"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " Mensagem codificada [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, array([ 1.1891e-01,  1.5255e-01, -8.2073e-02, -7.4144e-01,  7.5917e-01,\n       -4.8328e-01, -3.1009e-01,  5.1476e-01, -9.8708e-01,  6.1757e-04,\n       -1.5043e-01,  8.3770e-01, -1.0797e+00, -5.1460e-01,  1.3188e+00,\n        6.2007e-01,  1.3779e-01,  4.7108e-01, -7.2874e-02, -7.2675e-01,\n       -7.4116e-01,  7.5263e-01,  8.8180e-01,  2.9561e-01,  1.3548e+00,\n       -2.5701e+00, -1.3523e+00,  4.5880e-01,  1.0068e+00, -1.1856e+00,\n        3.4737e+00,  7.7898e-01, -7.2929e-01,  2.5102e-01, -2.6156e-01,\n       -3.4684e-01,  5.5841e-01,  7.5098e-01,  4.9830e-01, -2.6823e-01,\n       -2.7443e-03, -1.8298e-02, -2.8096e-01,  5.5318e-01,  3.7706e-02,\n        1.8555e-01, -1.5025e-01, -5.7512e-01, -2.6671e-01,  9.2121e-01],\n      dtype=float32), 0, 0, 0, 0, 0, 0, 0, 0, array([ 1.1891e-01,  1.5255e-01, -8.2073e-02, -7.4144e-01,  7.5917e-01,\n       -4.8328e-01, -3.1009e-01,  5.1476e-01, -9.8708e-01,  6.1757e-04,\n       -1.5043e-01,  8.3770e-01, -1.0797e+00, -5.1460e-01,  1.3188e+00,\n        6.2007e-01,  1.3779e-01,  4.7108e-01, -7.2874e-02, -7.2675e-01,\n       -7.4116e-01,  7.5263e-01,  8.8180e-01,  2.9561e-01,  1.3548e+00,\n       -2.5701e+00, -1.3523e+00,  4.5880e-01,  1.0068e+00, -1.1856e+00,\n        3.4737e+00,  7.7898e-01, -7.2929e-01,  2.5102e-01, -2.6156e-01,\n       -3.4684e-01,  5.5841e-01,  7.5098e-01,  4.9830e-01, -2.6823e-01,\n       -2.7443e-03, -1.8298e-02, -2.8096e-01,  5.5318e-01,  3.7706e-02,\n        1.8555e-01, -1.5025e-01, -5.7512e-01, -2.6671e-01,  9.2121e-01],\n      dtype=float32), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1] \n Comprimento: 53\n"
     ]
    }
   ],
   "source": [
    "msg_codificada_ex = dataset['X'].iloc[7]\n",
    "print(f' Mensagem codificada {msg_codificada_ex} \\n Comprimento: {len(msg_codificada_ex)}')"
   ]
  },
  {
   "source": [
    "### Agora, usaremos apenas as mensagens codificadas (vetores de variáveis categóricas, coluna do DataFrame 'X') e as saídas rotuladas em  das emoções  (coluna 'y' do DataFrame). Também vamos converter os objetos para arrays do numpy."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-250-bc7c0cfc6a55>:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n  X = np.vstack(dataset['X'].apply(lambda x: np.array(x)))\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((10017, 53), (53,), (10017, 1), (1,))"
      ]
     },
     "metadata": {},
     "execution_count": 250
    }
   ],
   "source": [
    "X = np.vstack(dataset['X'].apply(lambda x: np.array(x)))\n",
    "Y = np.array(dataset['emotion_code']).reshape(-1,1)\n",
    "X.shape, X[0].shape, Y.shape, Y[0].shape"
   ]
  },
  {
   "source": [
    "### Separando com train_test_split"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((8013, 53), (2004, 53))"
      ]
     },
     "metadata": {},
     "execution_count": 251
    }
   ],
   "source": [
    "train_size = 0.8    # percentual de exemplos para o treino\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,                       # dataset para ser dividido, entrada X e saída Y\n",
    "                                                    train_size=train_size,     # percentual resevado para o treinamento\n",
    "                                                    stratify=Y,                # estratificação para manter a distribuição dos rótulos igual entre treino e teste\n",
    "                                                    shuffle=True)              # embaralhar os exemplos aleatoriamente\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "source": [
    "### Criando o modelo classificador"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Torch_Mean_Layer(nn.Module):\n",
    "  '''Camada personalizada: calcula a média do tensor dentrada sobre a dimensão 1 (colunas).\n",
    "     Retorna um vetor linha, onde cada elemento é a média dos elementos da coluna correspondente do tensor de entrada.\n",
    "  '''\n",
    "  def forward(self, x, dim=1):\n",
    "    print(\"-----------------------\",x)\n",
    "    x = torch.mean(x, dim=dim, keepdims=True)\n",
    "    return x\n",
    "\n",
    "class mood_classifier(nn.Module):\n",
    "  '''Modelo classificador de emoções\n",
    "  '''\n",
    "\n",
    "  # ----------------------------------------------#\n",
    "  # Método construtor\n",
    "  def __init__(self, vocab_size, dim_embed, n_units): \n",
    "    super().__init__()  \n",
    "\n",
    "    embedding_seq = [] # \n",
    "    ann_seq       = [] # \n",
    "    soft_seq      = []\n",
    "\n",
    "    #---------------------------------------------------------------#\n",
    "    # Embedding step: sequência de operações para converter X --> h\n",
    "    embedding_seq.append(Torch_Mean_Layer())\n",
    "    #---------------------------------------------------------------#\n",
    "\n",
    "    #--------------------------------------------------------------------------#\n",
    "    # ANN: Rede Neural Artifical Tradicional, com regressão logística na saída\n",
    "    ann_seq.append(nn.Linear(dim_embed, n_units))\n",
    "    ann_seq.append(nn.ReLU(inplace=True))\n",
    "    ann_seq.append(nn.Linear(n_units, 6))\n",
    "    \n",
    "    #--------------------------------------------------------------------------#\n",
    "    # Softmax :)\n",
    "    soft_seq.append(nn.LogSoftmax(dim=1))\n",
    "\n",
    "    #--------------------------------------------------------------------------#\n",
    "\n",
    "    #--------------------------------------------------------------------------#\n",
    "    # \"merge\" de todas as camamadas em uma layer sequencial \n",
    "    # (uma sequência para cada etapa)\n",
    "    self.embedding = nn.Sequential(*embedding_seq)     # etapa de embedding \n",
    "    self.ann       = nn.Sequential(*ann_seq)           # etapa ANN\n",
    "    self.soft      = nn.Sequential(*soft_seq)\n",
    "    #--------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "  def forward(self, x): \n",
    "    '''Processamento realizado ao chamar y=modelo(x)\n",
    "    '''\n",
    "    x = self.embedding(x)  # aplica a etapa de embedding\n",
    "    x = self.ann(x)        # passa o embedding médio pelas camadas da ANN\n",
    "    x = x.view(-1,6)\n",
    "    x = self.soft(x)\n",
    "    return x  #Adcionar o softmax"
   ]
  },
  {
   "source": [
    "### Função que irá treinar "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, data, max_epochs = 1000, print_iters = 5):\n",
    "  X_train, Y_train = data\n",
    "  losses = []\n",
    "  accs = []\n",
    "  for i in range(max_epochs): # para cada época\n",
    "\n",
    "      #-----------------------------------#\n",
    "      # INÍCIO DO WORKFLOW DO TREINAMENTO #\n",
    "      # \n",
    "      Y_pred = model.forward(X_train)         # apresente os dados de entrada para o modelo, e obtenha a previsão    \n",
    "      loss = criterion(Y_pred.view(-1, 6), Y_train.view(-1))       # calcule a perda (o custo, o erro)\n",
    "      optimizer.zero_grad()                   # inicialize os gradientes\n",
    "      loss.backward()                         # backpropagation sobre a perda atual (cálculo dos novos gradientes) \n",
    "      optimizer.step()                        # atualização dos parâmetros da rede utilizando a regra do otimizador escolhido\n",
    "      # FIM DO WORKFLOW DO TREINAMENTO    #\n",
    "      #-----------------------------------#\n",
    "\n",
    "      # ------ Bloco Opcional ------ #\n",
    "      # Salvando métricas\n",
    "      losses.append(loss)                     # salvando a perda atual\n",
    "      acc = calc_accuracy(Y_pred, Y_train)     # calcula a taxa de acerto atual\n",
    "      accs.append(acc)\n",
    "      \n",
    "      # Imprimindo resultados parciais\n",
    "      if i % print_iters ==0: # a cada 10 iterações\n",
    "        print(f'epoch: {i:2}  loss: {loss.item():10.8f}') \n",
    "      #-----------------------------------#\n",
    "\n",
    "  #----------------------------------------------------------------------------# \n",
    "  print('\\n# Finished training!')\n",
    "  print(f'# --> epoch: {i}  \\n# --> initial loss: {losses[0]:10.8f} ,  \\n# --> accuracy: {acc:2.8f} , \\n# --> final loss: {losses[-1]:10.8f}')\n",
    "  \n",
    "  # retornando resultados\n",
    "  return model, losses, accs\n",
    "\n",
    "# Redefinindo cálculo da taxa de acerto \n",
    "def calc_accuracy(y_pred, y_true):\n",
    "  ''' Helper function para calcular a taxa de acerto deste exemplo.\n",
    "  '''\n",
    "  y_pred = torch.argmax(y_pred, dim=1)\n",
    "  y_pred = y_pred.float()\n",
    "  y_true = torch.squeeze(y_true) # tentar rexplicar dps\n",
    "  y_pred = torch.squeeze(y_pred)\n",
    "  num_hits  = torch.sum(y_pred==y_true).numpy()\n",
    "  num_total =  float(y_true.numel())\n",
    "  acc=  num_hits/num_total\n",
    "  return acc"
   ]
  },
  {
   "source": [
    "### Treinando com o modelo\n",
    "\n",
    "Convertendo os dados para tensores, instanciar o modelo, definir a função custo e o otimizador"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-189-327176538775>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmood_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim_embed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_units\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "data_train = (torch.LongTensor(X_train), torch.FloatTensor(Y_train))\n",
    "\n",
    "Model = mood_classifier(vocab_size=len(word2idx), dim_embed=50, n_units=100)\n",
    "print(Model)\n",
    "\n",
    "criterion = nn.NLLLoss() # cross entropy loss\n",
    "optimizer = torch.optim.Adam(Model.parameters(), lr = 0.01) \n",
    "\n",
    "Model, losses, accs = train_loop(Model, data_train, max_epochs=330, print_iters=1) # note que o modelo é sobrescrito pela saída treinada"
   ]
  },
  {
   "source": [
    "### Mostrando a *Confusion Matrix*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.5)\n",
    "\n",
    "matriz_de_confusao = confusion_matrix(Y_pred, Y_test)\n",
    "\n",
    "#criando list com as emoções\n",
    "emotion_class = ['Angry','Disgust','Fear','Happy','Sad','Surprise']\n",
    "\n",
    "df_matriz_de_confusao = pd.DataFrame(matriz_de_confusao, emotion_class, emotion_class)\n",
    "\n",
    "plot_conf_mat(Y_test, Y_pred)"
   ]
  },
  {
   "source": [
    "### Gráfico de *loss* e *accuracy*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_and_accuracy(losses, accs)"
   ]
  }
 ]
}