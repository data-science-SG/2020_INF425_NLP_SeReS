{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd087208729ed90ceac3456c3150dcf8f8574a040aaeff28d3df5566e1287b044ae",
   "display_name": "Python 3.7.9 64-bit ('machinelearning': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "# from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\fonta\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\fonta\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Setup some variables\n",
    "tknzr = TweetTokenizer()\n",
    "# stemmer = PorterStemmer()\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "source": [
    "----------------------------------------------------------------------"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Declaring some functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# função de limpar texto\n",
    "def cleanText(words, stem=False):\n",
    "  \"\"\"\n",
    "    Esta função recebe um text e retorna o mesmo, já tratado com stopwords & punctuation\n",
    "  \"\"\"\n",
    "  newWords = list()\n",
    "  for word in words:\n",
    "    word = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', word)\n",
    "    word = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", word)\n",
    "    word = word.lower()\n",
    "    if len(word) > 0 and word not in string.punctuation and word not in stopwords and word != \"<br />\":\n",
    "      if stem:\n",
    "        word = stemmer.stem(word)\n",
    "        newWords.append(word)\n",
    "      else:\n",
    "        newWords.append(word)\n",
    "  return newWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix daora\n",
    "def plot_conf_mat(y_test, y_preds, norm=\"false\"):\n",
    "    fig, ax = plt.subplots(figsize=(3, 3))\n",
    "    ax = sns.heatmap(confusion_matrix(y_test, y_preds, normalize=norm),\n",
    "                    annot=True,\n",
    "                    cbar=False)\n",
    "    plt.xlabel(\"True label\")\n",
    "    plt.ylabel(\"Predicted label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_and_accuracy(losses, accs):\n",
    "\n",
    "  fig, ax_tuple = plt.subplots(1, 2, figsize=(16,6))\n",
    "  fig.suptitle('Loss and accuracy')\n",
    "\n",
    "  for i, (y_label, y_values) in enumerate(zip(['BCE loss','Accuracy'],[losses, accs])):\n",
    "    ax_tuple[i].plot(range(len(y_values)),  y_values, label='train')\n",
    "    ax_tuple[i].set_xlabel('epochs')\n",
    "    ax_tuple[i].set_ylabel(y_label)\n",
    "    ax_tuple[i].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label2Embedding(word):\n",
    "  ''' Recebe uma string (word) e devolve o embedding vector correspondente (se existir).\n",
    "  '''\n",
    "  if word in modelo.vocab:\n",
    "    embed = modelo.get_vector(word)\n",
    "    if embed is not None:\n",
    "      return embed"
   ]
  },
  {
   "source": [
    "-------------------------------------------------------------------------------"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Importando o arquivo do GloVE de 50 dimensões e criando a variável *modelo* que será por onde iremos interagir com o *word embedding* já treinado"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'d:\\\\Dropbox\\\\GitHub\\\\2020_INF425_NLP_SeReS\\\\eda\\\\exercises'"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dims = 50\n",
    "\n",
    "glove_file = datapath(cwd+'/../data/glove.6B.50d.txt')\n",
    "tmp_file   = get_tmpfile(cwd+\"/../data/glove.6B.50d_word2vec.txt\")\n",
    "_          = glove2word2vec(glove_file, tmp_file)\n",
    "\n",
    "filename_txt = cwd+\"/../data/glove.6B.50d_word2vec.txt\"\n",
    "modelo = KeyedVectors.load_word2vec_format(filename_txt)"
   ]
  },
  {
   "source": [
    "### Lendo o dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Sl no                                             Tweets     Search key  \\\n",
       "0      1   #1: @fe ed \"RT @MirayaDizon1: Time is ticking...  happy moments   \n",
       "1      2   #2: @蓮花 &はすか ed \"RT @ninjaryugo: ＃コナモンの日 だそうで...  happy moments   \n",
       "2      3   #3: @Ris ♡ ed \"Happy birthday to one smokin h...  happy moments   \n",
       "3      4   #4: @월월 [씍쯴사랑로봇] jwinnie is the best, cheer u...  happy moments   \n",
       "4      5   #5: @Madhurima wth u vc♥ ed \"Good morning dea...  happy moments   \n",
       "\n",
       "  Feeling  \n",
       "0   happy  \n",
       "1   happy  \n",
       "2   happy  \n",
       "3   happy  \n",
       "4   happy  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sl no</th>\n      <th>Tweets</th>\n      <th>Search key</th>\n      <th>Feeling</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>#1: @fe ed \"RT @MirayaDizon1: Time is ticking...</td>\n      <td>happy moments</td>\n      <td>happy</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>#2: @蓮花 &amp;はすか ed \"RT @ninjaryugo: ＃コナモンの日 だそうで...</td>\n      <td>happy moments</td>\n      <td>happy</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>#3: @Ris ♡ ed \"Happy birthday to one smokin h...</td>\n      <td>happy moments</td>\n      <td>happy</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>#4: @월월 [씍쯴사랑로봇] jwinnie is the best, cheer u...</td>\n      <td>happy moments</td>\n      <td>happy</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>#5: @Madhurima wth u vc♥ ed \"Good morning dea...</td>\n      <td>happy moments</td>\n      <td>happy</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/data.csv\", encoding = \"utf-8\")\n",
    "df.head()"
   ]
  },
  {
   "source": [
    "### Copiando o dataset e fazendo transformações necessárias"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                              Tweets Feeling\n",
       "0   #1: @fe ed \"RT @MirayaDizon1: Time is ticking...   happy\n",
       "1   #2: @蓮花 &はすか ed \"RT @ninjaryugo: ＃コナモンの日 だそうで...   happy\n",
       "2   #3: @Ris ♡ ed \"Happy birthday to one smokin h...   happy\n",
       "3   #4: @월월 [씍쯴사랑로봇] jwinnie is the best, cheer u...   happy\n",
       "4   #5: @Madhurima wth u vc♥ ed \"Good morning dea...   happy"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweets</th>\n      <th>Feeling</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>#1: @fe ed \"RT @MirayaDizon1: Time is ticking...</td>\n      <td>happy</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>#2: @蓮花 &amp;はすか ed \"RT @ninjaryugo: ＃コナモンの日 だそうで...</td>\n      <td>happy</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>#3: @Ris ♡ ed \"Happy birthday to one smokin h...</td>\n      <td>happy</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>#4: @월월 [씍쯴사랑로봇] jwinnie is the best, cheer u...</td>\n      <td>happy</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>#5: @Madhurima wth u vc♥ ed \"Good morning dea...</td>\n      <td>happy</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "dataset = df.copy()\n",
    "\n",
    "dataset = dataset.drop(columns=[\"Sl no\", \"Search key\"])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "happy       39.213337\n",
       "sad         28.441649\n",
       "angry       13.387242\n",
       "fear         8.615354\n",
       "disgust      6.359189\n",
       "surprise     3.983229\n",
       "Name: Feeling, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "dataset['Feeling'].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONCATENAR COM O OUTRO DATASET"
   ]
  },
  {
   "source": [
    "### Criando uma categoria com o *pd.Categorical* para cada emoção na tabela de emoções, dessa forma, teremos algo do tipo: happy - 1 | angry - 2 | sad = 3\n",
    "\n",
    "Aplicando essas categorias na coluna *emotion_code*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10017 entries, 0 to 10016\nData columns (total 2 columns):\n #   Column   Non-Null Count  Dtype \n---  ------   --------------  ----- \n 0   Tweets   10017 non-null  object\n 1   Feeling  10017 non-null  object\ndtypes: object(2)\nmemory usage: 156.6+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                              Tweets Feeling  emotion_code\n",
       "0   #1: @fe ed \"RT @MirayaDizon1: Time is ticking...   happy             3\n",
       "1   #2: @蓮花 &はすか ed \"RT @ninjaryugo: ＃コナモンの日 だそうで...   happy             3\n",
       "2   #3: @Ris ♡ ed \"Happy birthday to one smokin h...   happy             3\n",
       "3   #4: @월월 [씍쯴사랑로봇] jwinnie is the best, cheer u...   happy             3\n",
       "4   #5: @Madhurima wth u vc♥ ed \"Good morning dea...   happy             3"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweets</th>\n      <th>Feeling</th>\n      <th>emotion_code</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>#1: @fe ed \"RT @MirayaDizon1: Time is ticking...</td>\n      <td>happy</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>#2: @蓮花 &amp;はすか ed \"RT @ninjaryugo: ＃コナモンの日 だそうで...</td>\n      <td>happy</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>#3: @Ris ♡ ed \"Happy birthday to one smokin h...</td>\n      <td>happy</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>#4: @월월 [씍쯴사랑로봇] jwinnie is the best, cheer u...</td>\n      <td>happy</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>#5: @Madhurima wth u vc♥ ed \"Good morning dea...</td>\n      <td>happy</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "dataset['Feeling'] = pd.Categorical(dataset['Feeling'])\n",
    "dataset['emotion_code'] = dataset['Feeling'].cat.codes\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10017 entries, 0 to 10016\nData columns (total 3 columns):\n #   Column        Non-Null Count  Dtype   \n---  ------        --------------  -----   \n 0   Tweets        10017 non-null  object  \n 1   Feeling       10017 non-null  category\n 2   emotion_code  10017 non-null  int8    \ndtypes: category(1), int8(1), object(1)\nmemory usage: 98.2+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Feeling   emotion_code\n",
       "angry     0               1341\n",
       "disgust   1                637\n",
       "fear      2                863\n",
       "happy     3               3928\n",
       "sad       4               2849\n",
       "surprise  5                399\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "dataset[['Feeling','emotion_code']].value_counts().sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "type(dataset['Tweets'][0])"
   ]
  },
  {
   "source": [
    "### Tokenizando os tweets com o *tknzr.tokenize*, e logo após, limpando os tokens com a função *cleanText*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                              Tweets Feeling  emotion_code  \\\n",
       "0   #1: @fe ed \"RT @MirayaDizon1: Time is ticking...   happy             3   \n",
       "1   #2: @蓮花 &はすか ed \"RT @ninjaryugo: ＃コナモンの日 だそうで...   happy             3   \n",
       "2   #3: @Ris ♡ ed \"Happy birthday to one smokin h...   happy             3   \n",
       "3   #4: @월월 [씍쯴사랑로봇] jwinnie is the best, cheer u...   happy             3   \n",
       "4   #5: @Madhurima wth u vc♥ ed \"Good morning dea...   happy             3   \n",
       "\n",
       "                                           CleanText  \n",
       "0  [1, ed, rt, time, ticking, fast, relive, past,...  \n",
       "1  [2, @蓮花, はすか, ed, rt, ＃, コナモンの日, だそうで, 、, コナモン...  \n",
       "2  [3, ♡, ed, happy, birthday, one, smokin, hot, ...  \n",
       "3  [4, @월월, 씍쯴사랑로봇, jwinnie, best, cheer, jwinnie...  \n",
       "4  [5, wth, u, vc, ♥, ed, good, morning, dear, ❤,...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweets</th>\n      <th>Feeling</th>\n      <th>emotion_code</th>\n      <th>CleanText</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>#1: @fe ed \"RT @MirayaDizon1: Time is ticking...</td>\n      <td>happy</td>\n      <td>3</td>\n      <td>[1, ed, rt, time, ticking, fast, relive, past,...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>#2: @蓮花 &amp;はすか ed \"RT @ninjaryugo: ＃コナモンの日 だそうで...</td>\n      <td>happy</td>\n      <td>3</td>\n      <td>[2, @蓮花, はすか, ed, rt, ＃, コナモンの日, だそうで, 、, コナモン...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>#3: @Ris ♡ ed \"Happy birthday to one smokin h...</td>\n      <td>happy</td>\n      <td>3</td>\n      <td>[3, ♡, ed, happy, birthday, one, smokin, hot, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>#4: @월월 [씍쯴사랑로봇] jwinnie is the best, cheer u...</td>\n      <td>happy</td>\n      <td>3</td>\n      <td>[4, @월월, 씍쯴사랑로봇, jwinnie, best, cheer, jwinnie...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>#5: @Madhurima wth u vc♥ ed \"Good morning dea...</td>\n      <td>happy</td>\n      <td>3</td>\n      <td>[5, wth, u, vc, ♥, ed, good, morning, dear, ❤,...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "dataset[\"CleanText\"] = [tknzr.tokenize(sentence) for sentence in dataset[\"Tweets\"]]\n",
    "dataset[\"CleanText\"] = [cleanText(sentence) for sentence in dataset[\"CleanText\"]]\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['CleanText'][50]"
   ]
  },
  {
   "source": [
    "### Conversão dos textos em uma sequência de índices (correspondente ao token do texto)\n",
    "\n",
    "Limitar o tamanho máximo de um texto com ***max_len*** (truncar mensagem) e completar com <PAD> todos os textos que não cumprirem esse tamanho, para então termos variáveis do mesmo tamanho"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_len = 150        # comprimento máximo da mensagem (em número de palavras)\n",
    "encoded_docs = []    # inicializa a lista de documentos codificados\n",
    "\n",
    "for sentence in dataset['CleanText']: # para cada token\n",
    "  encoded_d = [label2Embedding(t) for t in sentence]\n",
    "  encoded_d = [vec.tolist() for vec in encoded_d if vec is not None]\n",
    "\n",
    "  # adiciona o padding, se necessário\n",
    "  padding_word_vecs = [np.zeros(num_dims).tolist()]*max(0, max_len-len(encoded_d)) \n",
    "  encoded_d = padding_word_vecs + encoded_d\n",
    "  \n",
    "  # trunca o documento e salva na lista de documentos codificados\n",
    "  encoded_docs.append(encoded_d[:max_len]) \n",
    "\n",
    "\n",
    "encoded_docs_arrays = [np.vstack(sentence) for sentence in encoded_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(list, numpy.ndarray, (150, 50))"
      ]
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": [
    "type(encoded_docs_arrays), type(encoded_docs_arrays[0]), encoded_docs_arrays[0].shape"
   ]
  },
  {
   "source": [
    "### Agora os textos são os documentos codificados"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10012    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
       "10013    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
       "10014    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
       "10015    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
       "10016    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
       "Name: X, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "source": [
    "dataset['X'] = pd.Series(encoded_docs_arrays)\n",
    "dataset['X'].tail() #.iloc[-1].shape"
   ]
  },
  {
   "source": [
    "### Verificando um exemplo de uma frase codificada"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((150, 50), dtype('float64'))"
      ]
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "source": [
    "dataset['X'][0].shape, dataset['X'][0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " Mensagem codificada [[ 0.          0.          0.         ...  0.          0.\n   0.        ]\n [ 0.          0.          0.         ...  0.          0.\n   0.        ]\n [ 0.          0.          0.         ...  0.          0.\n   0.        ]\n ...\n [ 0.359       0.72350001  0.15195    ... -0.53805    -1.07599998\n   0.24003001]\n [ 0.067025   -0.010427    0.61778003 ... -0.42741001 -0.19243\n   0.42443001]\n [-0.29115999  1.17639995 -0.20423999 ... -0.58471     0.64288998\n   0.15782   ]] \n Comprimento: 150 Dimensões: 50\n"
     ]
    }
   ],
   "source": [
    "msg_codificada_ex = dataset['X'].iloc[7]\n",
    "print(f' Mensagem codificada {msg_codificada_ex} \\n Comprimento: {msg_codificada_ex.shape[0]} Dimensões: {msg_codificada_ex.shape[1]}')"
   ]
  },
  {
   "source": [
    "### Agora, usaremos apenas as mensagens codificadas (vetores de variáveis categóricas, coluna do DataFrame 'X') e as saídas rotuladas em  das emoções  (coluna 'y' do DataFrame). Também vamos converter os objetos para arrays do numpy."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((10017, 150, 50), (150, 50), (10017, 1), (1,))"
      ]
     },
     "metadata": {},
     "execution_count": 101
    }
   ],
   "source": [
    "X = np.dstack(dataset['X'].values).transpose(2,0,1)\n",
    "Y = dataset['emotion_code'].values.reshape(-1,1)\n",
    "X.shape, X[0].shape, Y.shape, Y[0].shape"
   ]
  },
  {
   "source": [
    "### Separando com train_test_split"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8    # percentual de exemplos para o treino\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,                       # dataset para ser dividido, entrada X e saída Y\n",
    "                                                    train_size=train_size,     # percentual resevado para o treinamento\n",
    "                                                    stratify=Y,                # estratificação para manter a distribuição dos rótulos igual entre treino e teste\n",
    "                                                    shuffle=True)              # embaralhar os exemplos aleatoriamente\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "source": [
    "### Criando o modelo classificador"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Torch_Mean_Layer(nn.Module):\n",
    "  '''Camada personalizada: calcula a média do tensor dentrada sobre a dimensão 1 (colunas).\n",
    "     Retorna um vetor linha, onde cada elemento é a média dos elementos da coluna correspondente do tensor de entrada.\n",
    "  '''\n",
    "  def forward(self, x, dim=1):\n",
    "    # print(\"-----------------------\",x)\n",
    "    x = torch.mean(x, dim=dim, keepdims=True)\n",
    "    return x\n",
    "\n",
    "class moodClassifier(nn.Module):\n",
    "  '''Modelo classificador de emoções\n",
    "  '''\n",
    "\n",
    "  # ----------------------------------------------#\n",
    "  # Método construtor\n",
    "  def __init__(self, dim_embed, n_units): \n",
    "    super().__init__()  \n",
    "\n",
    "    embedding_seq = [] # \n",
    "    ann_seq       = [] # \n",
    "    soft_seq      = []\n",
    "\n",
    "    #---------------------------------------------------------------#\n",
    "    # Embedding step: sequência de operações para converter X --> h\n",
    "    embedding_seq.append(Torch_Mean_Layer())\n",
    "    #---------------------------------------------------------------#\n",
    "\n",
    "    #--------------------------------------------------------------------------#\n",
    "    # ANN: Rede Neural Artifical Tradicional, com regressão logística na saída\n",
    "    ann_seq.append(nn.Linear(dim_embed, n_units))\n",
    "    ann_seq.append(nn.ReLU(inplace=True))\n",
    "    ann_seq.append(nn.Linear(n_units, 6))\n",
    "    \n",
    "    #--------------------------------------------------------------------------#\n",
    "    # Softmax :)\n",
    "    soft_seq.append(nn.LogSoftmax(dim=1))\n",
    "\n",
    "    #--------------------------------------------------------------------------#\n",
    "\n",
    "    #--------------------------------------------------------------------------#\n",
    "    # \"merge\" de todas as camamadas em uma layer sequencial \n",
    "    # (uma sequência para cada etapa)\n",
    "    self.embedding = nn.Sequential(*embedding_seq)     # etapa de embedding \n",
    "    self.ann       = nn.Sequential(*ann_seq)           # etapa ANN\n",
    "    self.soft      = nn.Sequential(*soft_seq)\n",
    "    #--------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "  def forward(self, x): \n",
    "    '''Processamento realizado ao chamar y=modelo(x)\n",
    "    '''\n",
    "    x = self.embedding(x)  # aplica a etapa de embedding\n",
    "    x = self.ann(x)        # passa o embedding médio pelas camadas da ANN\n",
    "    x = x.view(-1,6)\n",
    "    x = self.soft(x)\n",
    "    return x  #Adcionar o softmax"
   ]
  },
  {
   "source": [
    "### Função que irá treinar "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, data, max_epochs = 1000, print_iters = 5):\n",
    "  X_train, Y_train = data\n",
    "  losses = []\n",
    "  accs = []\n",
    "  for i in range(max_epochs): # para cada época\n",
    "\n",
    "      #-----------------------------------#\n",
    "      # INÍCIO DO WORKFLOW DO TREINAMENTO #\n",
    "      # \n",
    "      Y_pred = model.forward(X_train)         # apresente os dados de entrada para o modelo, e obtenha a previsão    \n",
    "      loss = criterion(Y_pred.view(-1, 6), Y_train.view(-1))       # calcule a perda (o custo, o erro)\n",
    "      optimizer.zero_grad()                   # inicialize os gradientes\n",
    "      loss.backward()                         # backpropagation sobre a perda atual (cálculo dos novos gradientes) \n",
    "      optimizer.step()                        # atualização dos parâmetros da rede utilizando a regra do otimizador escolhido\n",
    "      # FIM DO WORKFLOW DO TREINAMENTO    #\n",
    "      #-----------------------------------#\n",
    "\n",
    "      # ------ Bloco Opcional ------ #\n",
    "      # Salvando métricas\n",
    "      losses.append(loss)                     # salvando a perda atual\n",
    "      acc = calc_accuracy(Y_pred, Y_train)     # calcula a taxa de acerto atual\n",
    "      accs.append(acc)\n",
    "      \n",
    "      # Imprimindo resultados parciais\n",
    "      if i % print_iters ==0: # a cada 10 iterações\n",
    "        print(f'epoch: {i:2}  loss: {loss.item():10.8f}') \n",
    "      #-----------------------------------#\n",
    "\n",
    "  #----------------------------------------------------------------------------# \n",
    "  print('\\n# Finished training!')\n",
    "  print(f'# --> epoch: {i}  \\n# --> initial loss: {losses[0]:10.8f} ,  \\n# --> accuracy: {acc:2.8f} , \\n# --> final loss: {losses[-1]:10.8f}')\n",
    "  \n",
    "  # retornando resultados\n",
    "  return model, losses, accs\n",
    "\n",
    "# Redefinindo cálculo da taxa de acerto \n",
    "def calc_accuracy(y_pred, y_true):\n",
    "  ''' Helper function para calcular a taxa de acerto deste exemplo.\n",
    "  '''\n",
    "  y_pred = torch.argmax(y_pred, dim=1)\n",
    "  y_pred = y_pred.float()\n",
    "  y_true = torch.squeeze(y_true) # tentar rexplicar dps\n",
    "  y_pred = torch.squeeze(y_pred)\n",
    "  num_hits  = torch.sum(y_pred==y_true).numpy()\n",
    "  num_total =  float(y_true.numel())\n",
    "  acc=  num_hits/num_total\n",
    "  return acc"
   ]
  },
  {
   "source": [
    "### Treinando com o modelo\n",
    "\n",
    "Convertendo os dados para tensores, instanciar o modelo, definir a função custo e o otimizador"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "data_train = (torch.FloatTensor(X_train), torch.LongTensor(Y_train))\n",
    "\n",
    "Model = moodClassifier(dim_embed=50, n_units=500)\n",
    "print(Model)\n",
    "\n",
    "criterion = nn.NLLLoss() # cross entropy loss\n",
    "optimizer = torch.optim.Adam(Model.parameters(), lr = 0.001) \n",
    "\n",
    "Model, losses, accs = train_loop(Model, data_train, max_epochs=5000, print_iters=1) # note que o modelo é sobrescrito pela saída treinada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rodando o Modelo treinado com o Dataset de Teste\n",
    "data_test = (torch.FloatTensor(X_test), torch.LongTensor(Y_test))\n",
    "X_test_tensor, Y_test_tensor = data_test\n",
    "Y_pred_tensor = Model.forward(X_test_tensor) \n",
    "\n",
    "# hard-decision e pós-processamento da saída\n",
    "y_pred = torch.argmax(Y_pred_tensor, dim=1)\n",
    "y_pred = y_pred.float()\n",
    "y_true = torch.squeeze(Y_test_tensor) \n",
    "y_pred = torch.squeeze(y_pred)"
   ]
  },
  {
   "source": [
    "### Mostrando a *Confusion Matrix*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set(font_scale=1.5)\n",
    "fig, ax = plt.subplots(1,1,figsize=(11,11))\n",
    "\n",
    "cat_dict = dict( enumerate(dataset['Feeling'].cat.categories ) )\n",
    "\n",
    "y_pred_cats = [cat_dict[i] for i in y_pred.numpy().tolist()]\n",
    "y_true_cats = [cat_dict[i] for i in y_true.numpy().tolist()]\n",
    "\n",
    "matriz_de_confusao = 100*confusion_matrix(y_pred_cats, y_true_cats, normalize='true', labels=list(cat_dict.values()))\n",
    "# normalize{‘true’, ‘pred’, ‘all’}, default=None\n",
    "# Normalizes confusion matrix over the true (rows), predicted (columns) conditions or all the population. If None, confusion matrix will not be normalized.\n",
    "\n",
    "#criando list com as emoções\n",
    "emotion_class = ['Angry','Disgust','Fear','Happy','Sad','Surprise']\n",
    "\n",
    "df_matriz_de_confusao = pd.DataFrame(matriz_de_confusao, emotion_class, emotion_class)\n",
    "\n",
    "cfm = ConfusionMatrixDisplay(matriz_de_confusao)\n",
    "\n",
    "cfm.plot(ax=ax, cmap='RdBu')\n",
    "\n",
    "ax.set_xticklabels(labels=list(cat_dict.values()))\n",
    "ax.set_yticklabels(labels=list(cat_dict.values()))\n",
    "\n",
    "ax.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matriz_de_confusao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "source": [
    "### Gráfico de *loss* e *accuracy*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_and_accuracy(losses, accs)"
   ]
  },
  {
   "source": [
    "#### Salvando o modelo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Model, cwd+\"/../../models/emotions_classifier.pth\")"
   ]
  },
  {
   "source": [
    "#### Salvando o dict"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Model.state_dict(), cwd+\"/../../dicts/emotions_classifier_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(cwd+\"/../../models/emotions_classifier.pth\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}